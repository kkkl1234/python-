{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a027eea9-a1be-47ee-9b3e-da1338eabd7f",
   "metadata": {},
   "source": [
    "## æ•°æ®æ¸…æ´—åŠå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8933e869-135a-464e-bf49-483fa1f2f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# -------------------------- 1. å…¨å±€é…ç½®--------------------------\n",
    "# å­—ä½“ä¸æ˜¾ç¤ºé…ç½®ï¼ˆè§£å†³ä¸­æ–‡ä¹±ç ï¼‰\n",
    "plt.rcParams['font.sans-serif'] = ['Hiragino Sans GB', 'STHeiti', 'Arial']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "os.makedirs(\"visualization_results\", exist_ok=True)\n",
    "\n",
    "# å›ºå®šé…ç½®\n",
    "POWER_CONFIGS = [\n",
    "    (\"Zone 1 Power Consumption\", \"åŒºåŸŸ1ç”µåŠ›æ¶ˆè€— (kW)\", \"#45B7D1\"),\n",
    "    (\"Zone 2  Power Consumption\", \"åŒºåŸŸ2ç”µåŠ›æ¶ˆè€— (kW)\", \"#45B7D1\"),  # åˆ—åå«2ä¸ªç©ºæ ¼\n",
    "    (\"Zone 3  Power Consumption\", \"åŒºåŸŸ3ç”µåŠ›æ¶ˆè€— (kW)\", \"#45B7D1\")   # åˆ—åå«2ä¸ªç©ºæ ¼\n",
    "]\n",
    "FEATURE_COLS = [\n",
    "    'Temperature', 'Humidity', 'Wind Speed', \n",
    "    'general diffuse flows', 'diffuse flows'\n",
    "]\n",
    "UNIT_MAP = {\n",
    "    'Temperature': 'Â°C',\n",
    "    'Humidity': '%',\n",
    "    'Wind Speed': 'm/s',\n",
    "    'general diffuse flows': 'W/mÂ²',\n",
    "    'diffuse flows': 'W/mÂ²'\n",
    "}\n",
    "\n",
    "# -------------------------- 2. ç±»å®šä¹‰ --------------------------\n",
    "class PowerConsumptionAnalyzer:\n",
    "    def __init__(self, data_path: str = \"consumption.csv\"):\n",
    "        self.data_path = data_path\n",
    "        self.df_raw = None  # åŸå§‹æ•°æ®\n",
    "        self.df_processed = None  # æ—¥æœŸç´¢å¼•å¤„ç†åçš„æ•°æ®\n",
    "        self.df_daily_mean = None  # æ—¥å‡å€¼æ•°æ®\n",
    "        self.feature_data = None  # ç‰¹å¾æ•°æ®ï¼ˆç”¨äºåç»­ç®—æ³•ï¼‰\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        try:\n",
    "            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "            if not os.path.exists(self.data_path):\n",
    "                raise FileNotFoundError(f\"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{self.data_path}\")\n",
    "            \n",
    "            # è¯»å–æ•°æ®\n",
    "            self.df_raw = pd.read_csv(self.data_path)\n",
    "            print(f\"âœ… æˆåŠŸåŠ è½½åŸå§‹æ•°æ®ï¼Œå½¢çŠ¶ï¼š{self.df_raw.shape}\")\n",
    "            \n",
    "            # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨\n",
    "            required_cols = [\"DateTime\"] + [cfg[0] for cfg in POWER_CONFIGS] + FEATURE_COLS\n",
    "            missing_cols = [col for col in required_cols if col not in self.df_raw.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"æ•°æ®ç¼ºå¤±å¿…è¦åˆ—ï¼š{missing_cols}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼š{str(e)}\")\n",
    "\n",
    "    def process_data(self) -> None:\n",
    "        if self.df_raw is None:\n",
    "            raise ValueError(\"è¯·å…ˆè°ƒç”¨ load_data() åŠ è½½æ•°æ®\")\n",
    "        \n",
    "        try:\n",
    "            # æ—¥æœŸè½¬æ¢ä¸ç´¢å¼•è®¾ç½®\n",
    "            self.df_processed = self.df_raw.copy()\n",
    "            self.df_processed[\"DateTime\"] = pd.to_datetime(self.df_processed[\"DateTime\"], errors='coerce')\n",
    "            \n",
    "            # æ£€æŸ¥æ—¥æœŸè½¬æ¢æœ‰æ•ˆæ€§\n",
    "            invalid_date_count = self.df_processed[\"DateTime\"].isnull().sum()\n",
    "            if invalid_date_count > 0:\n",
    "                raise ValueError(f\"æ—¥æœŸè½¬æ¢å¤±è´¥ï¼Œå…±{invalid_date_count}æ¡æ— æ•ˆæ—¥æœŸæ•°æ®\")\n",
    "            \n",
    "            self.df_processed.set_index(\"DateTime\", inplace=True)\n",
    "            \n",
    "            # æ—¥å‡å€¼é‡é‡‡æ ·\n",
    "            self.df_daily_mean = self.df_processed.resample('D').mean()\n",
    "            \n",
    "            # æ£€æŸ¥é‡é‡‡æ ·åæ•°æ®æœ‰æ•ˆæ€§\n",
    "            if self.df_daily_mean.empty:\n",
    "                raise ValueError(\"æ—¥å‡å€¼é‡é‡‡æ ·åæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥åŸå§‹æ•°æ®æ—¶é—´èŒƒå›´\")\n",
    "            \n",
    "            # å¤„ç†ç‰¹å¾åˆ—ç¼ºå¤±å€¼ï¼ˆç”¨æˆ·æŒ‡å®šé€»è¾‘ï¼‰\n",
    "            self.feature_data = self.df_daily_mean[FEATURE_COLS].fillna(method='ffill').fillna(method='bfill')\n",
    "            \n",
    "            print(f\"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ\")\n",
    "            print(f\"  - å¤„ç†åæ•°æ®å½¢çŠ¶ï¼š{self.df_processed.shape}\")\n",
    "            print(f\"  - æ—¥å‡å€¼æ•°æ®å½¢çŠ¶ï¼š{self.df_daily_mean.shape}\")\n",
    "            print(f\"  - ç‰¹å¾æ•°æ®å½¢çŠ¶ï¼š{self.feature_data.shape}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼š{str(e)}\")\n",
    "\n",
    "    def plot_power_time_series(self) -> None:\n",
    "        \"\"\"ç»˜åˆ¶3ä¸ªåŒºåŸŸç”µåŠ›æ¶ˆè€—æ—¥å‡å€¼æ—¶åºå›¾\"\"\"\n",
    "        if self.df_daily_mean is None:\n",
    "            raise ValueError(\"è¯·å…ˆè°ƒç”¨ process_data() å¤„ç†æ•°æ®\")\n",
    "        \n",
    "        # åˆ›å»º3è¡Œ1åˆ—å­å›¾\n",
    "        fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(14, 12), sharex=True)\n",
    "        \n",
    "        # éå†æ¯ä¸ªåŒºåŸŸç»˜åˆ¶\n",
    "        for idx, (col_name, chinese_name, color) in enumerate(POWER_CONFIGS):\n",
    "            ax = axes[idx]\n",
    "            # ç»˜åˆ¶æ—¶åºæ›²çº¿ï¼ˆå«åœ†ç‚¹ä¼˜åŒ–ï¼‰\n",
    "            ax.plot(\n",
    "                self.df_daily_mean.index, self.df_daily_mean[col_name],\n",
    "                color=color, linewidth=2.5, alpha=0.9,\n",
    "                markersize=5, markerfacecolor='white', markeredgewidth=1.5\n",
    "            )\n",
    "            \n",
    "            # å›¾è¡¨æ ·å¼é…ç½®\n",
    "            ax.set_title(f\"{chinese_name}ï¼ˆæ—¥å‡å€¼æ—¶é—´åºåˆ—ï¼‰\", fontsize=15, fontweight=\"bold\", pad=15)\n",
    "            ax.set_ylabel(chinese_name.split(\" \")[-1], fontsize=11, fontweight=\"medium\")\n",
    "            ax.tick_params(axis='y', labelsize=10)\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            \n",
    "            # è°ƒæ•´yè½´èŒƒå›´ï¼ˆä¸Šä¸‹ç•™ç©º5%ï¼‰\n",
    "            y_min = self.df_daily_mean[col_name].min() * 0.95\n",
    "            y_max = self.df_daily_mean[col_name].max() * 1.05\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            \n",
    "            # æœ€åä¸€ä¸ªå­å›¾æ˜¾ç¤ºxè½´æ ‡ç­¾\n",
    "            if idx == 2:\n",
    "                ax.set_xlabel(\"æ—¥æœŸ\", fontsize=12, fontweight=\"medium\")\n",
    "                ax.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        \n",
    "        # è°ƒæ•´xè½´èŒƒå›´\n",
    "        x_min = self.df_daily_mean.index.min() - pd.Timedelta(days=1)\n",
    "        x_max = self.df_daily_mean.index.max() + pd.Timedelta(days=1)\n",
    "        for ax in axes:\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "        \n",
    "        # è°ƒæ•´å­å›¾é—´è·\n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.savefig(\"visualization_results/power_daily_mean_series.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"âœ… ç”µåŠ›æ¶ˆè€—æ—¶åºå›¾å·²ä¿å­˜è‡³ visualization_results/\")\n",
    "\n",
    "    def plot_feature_time_series(self) -> None:\n",
    "        \"\"\"ç»˜åˆ¶5ä¸ªç¯å¢ƒç‰¹å¾æ—¶åºå›¾\"\"\"\n",
    "        if self.feature_data is None:\n",
    "            raise ValueError(\"è¯·å…ˆè°ƒç”¨ process_data() å¤„ç†æ•°æ®\")\n",
    "        \n",
    "        # åˆ›å»º3è¡Œ2åˆ—å­å›¾\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(16, 10), sharex=True)\n",
    "        fig.suptitle('ç¯å¢ƒç‰¹å¾æ—¶åºå˜åŒ–è¶‹åŠ¿', fontsize=18, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # å­å›¾ä½ç½®æ˜ å°„ï¼ˆ5ä¸ªç‰¹å¾å¯¹åº”å‰5ä¸ªä½ç½®ï¼‰\n",
    "        subplot_pos = [(0,0), (0,1), (1,0), (1,1), (2,0)]\n",
    "        \n",
    "        # é€ä¸ªç»˜åˆ¶ç‰¹å¾\n",
    "        for idx, (feat, pos) in enumerate(zip(FEATURE_COLS, subplot_pos)):\n",
    "            ax = axes[pos]\n",
    "            # ç»˜åˆ¶æ—¶åºæ›²çº¿ï¼ˆé»˜è®¤é¢œè‰²ï¼‰\n",
    "            ax.plot(\n",
    "                self.feature_data.index, self.feature_data[feat],\n",
    "                linewidth=1.8, alpha=0.8, label=feat\n",
    "            )\n",
    "            # æ·»åŠ å‡å€¼çº¿\n",
    "            mean_val = self.feature_data[feat].mean()\n",
    "            ax.axhline(\n",
    "                y=mean_val, color='black', linestyle='--', linewidth=1.2, alpha=0.7,\n",
    "                label=f'å‡å€¼ï¼š{mean_val:.2f} {UNIT_MAP.get(feat, \"\")}'\n",
    "            )\n",
    "            \n",
    "            # æ ·å¼é…ç½®\n",
    "            ax.set_title(feat, fontsize=12, fontweight='medium', pad=8)\n",
    "            ax.set_ylabel(f'æ•°å€¼ï¼ˆ{UNIT_MAP.get(feat, \"æ— å•ä½\")}ï¼‰', fontsize=10)\n",
    "            ax.grid(True, linestyle='--', alpha=0.3)\n",
    "            ax.legend(loc='upper right', fontsize=9, framealpha=0.9)\n",
    "            ax.tick_params(axis='y', labelsize=9)\n",
    "        \n",
    "        # éšè—æœ€åä¸€ä¸ªç©ºå­å›¾\n",
    "        axes[2, 1].axis('off')\n",
    "        \n",
    "        # é…ç½®xè½´æ ‡ç­¾\n",
    "        axes[2, 0].set_xlabel('æ—¶é—´', fontsize=12, fontweight='medium')\n",
    "        axes[2, 0].tick_params(axis='x', rotation=45, labelsize=9)\n",
    "        \n",
    "        # è°ƒæ•´é—´è·ï¼ˆé¢„ç•™æ ‡é¢˜ç©ºé—´ï¼‰\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        # ä¿å­˜å›¾ç‰‡\n",
    "        plt.savefig(\"visualization_results/feature_time_series.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"âœ… ç‰¹å¾æ—¶åºå›¾å·²ä¿å­˜è‡³ visualization_results/\")\n",
    "\n",
    "    def get_processed_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"è·å–å¤„ç†åçš„æ•°æ®\"\"\"\n",
    "        if self.df_daily_mean is None or self.feature_data is None:\n",
    "            raise ValueError(\"è¯·å…ˆè°ƒç”¨ process_data() å¤„ç†æ•°æ®\")\n",
    "        return self.df_daily_mean, self.feature_data\n",
    "\n",
    "# -------------------------- 3. è¾…åŠ©å‡½æ•°ï¼ˆæ»¡è¶³å‡½æ•°è°ƒç”¨è¦æ±‚ï¼‰ --------------------------\n",
    "def validate_data_quality(df: pd.DataFrame, data_name: str) -> bool:\n",
    "    \"\"\"éªŒè¯æ•°æ®è´¨é‡ï¼ˆç‹¬ç«‹å‡½æ•°ï¼Œä½“ç°å‡½æ•°è°ƒç”¨ï¼‰\"\"\"\n",
    "    # æ£€æŸ¥æ•°æ®éç©º\n",
    "    if df.empty:\n",
    "        print(f\"âŒ {data_name} æ•°æ®ä¸ºç©º\")\n",
    "        return False\n",
    "    # æ£€æŸ¥ç‰¹å¾åˆ—ç¼ºå¤±å€¼\n",
    "    missing_ratio = df[FEATURE_COLS].isnull().sum() / len(df) * 100\n",
    "    high_missing_cols = missing_ratio[missing_ratio > 30].index.tolist()\n",
    "    if high_missing_cols:\n",
    "        print(f\"âš ï¸ {data_name} ä»¥ä¸‹ç‰¹å¾ç¼ºå¤±ç‡è¶…è¿‡30%ï¼š{high_missing_cols}\")\n",
    "    return True\n",
    "\n",
    "# -------------------------- 4. ä¸»å‡½æ•° --------------------------\n",
    "def main():\n",
    "    \n",
    "    try:\n",
    "        # 1. åˆå§‹åŒ–åˆ†æå™¨\n",
    "        analyzer = PowerConsumptionAnalyzer(data_path=\"consumption.csv\")\n",
    "        \n",
    "        # 2. åŠ è½½æ•°æ®\n",
    "        analyzer.load_data()\n",
    "        \n",
    "        # 3. æ•°æ®é¢„å¤„ç†\n",
    "        analyzer.process_data()\n",
    "        \n",
    "        # 4. éªŒè¯æ•°æ®è´¨é‡\n",
    "        df_daily_mean, feature_data = analyzer.get_processed_data()\n",
    "        if validate_data_quality(df_daily_mean, \"æ—¥å‡å€¼æ•°æ®\") and validate_data_quality(feature_data, \"ç‰¹å¾æ•°æ®\"):\n",
    "            print(\"âœ… æ•°æ®è´¨é‡éªŒè¯é€šè¿‡\")\n",
    "        \n",
    "        # 5. ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨\n",
    "        print(\"\\nğŸ“Š å¼€å§‹ç»˜åˆ¶ç”µåŠ›æ¶ˆè€—æ—¶åºå›¾...\")\n",
    "        analyzer.plot_power_time_series()\n",
    "        \n",
    "        print(\"\\nğŸ“Š å¼€å§‹ç»˜åˆ¶ç‰¹å¾æ—¶åºå›¾...\")\n",
    "        analyzer.plot_feature_time_series()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ‰ æ•°æ®é¢„å¤„ç†ä¸å¯è§†åŒ–æµç¨‹å®Œæˆï¼\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"è¾“å‡ºæ–‡ä»¶æ¸…å•ï¼š\")\n",
    "        print(\"1. ç”µåŠ›æ¶ˆè€—æ—¶åºå›¾ï¼švisualization_results/power_daily_mean_series.png\")\n",
    "        print(\"2. ç‰¹å¾æ—¶åºå›¾ï¼švisualization_results/feature_time_series.png\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ æµç¨‹æ‰§è¡Œå¤±è´¥ï¼š{str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0414b-013a-4bb3-8bb5-8b2a79adc562",
   "metadata": {},
   "source": [
    "## ARIMAæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdfd8f5-6447-44d7-a8a3-3856ba2cc86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# åŸºç¡€é…ç½®\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "plt.rcParams['font.sans-serif'] = ['Hiragino Sans GB', 'STHeiti', 'Arial']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è¯„ä¼°æŒ‡æ ‡å‡½æ•°\n",
    "def calculate_mape(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true_safe = np.where(y_true == 0, eps, y_true)\n",
    "    mape = np.mean(np.abs((y_true_safe - y_pred) / y_true_safe)) * 100\n",
    "    return round(mape, 2)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "    mape = calculate_mape(y_true, y_pred)\n",
    "    return round(mae, 2), round(rmse, 2), mape\n",
    "\n",
    "# æ ¸å¿ƒç±»å®šä¹‰\n",
    "class ARIMATimeSeriesAnalyzer:\n",
    "    def __init__(self, data_path, target_col):\n",
    "        self.data_path = data_path\n",
    "        self.target_col = target_col\n",
    "        self.df = None\n",
    "        self.df_daily_mean = None\n",
    "        self.target_series = None\n",
    "        self.train_series = None\n",
    "        self.test_series = None\n",
    "        self.best_order = None\n",
    "        self.model = None\n",
    "        self.model_result = None\n",
    "\n",
    "    def load_and_preprocess(self):\n",
    "        \"\"\"åŠ è½½æ•°æ®å¹¶é¢„å¤„ç†ï¼ˆæ—¥å‡å€¼é‡é‡‡æ ·ï¼‰\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self.data_path):\n",
    "                raise FileNotFoundError(f\"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{self.data_path}\")\n",
    "            \n",
    "            self.df = pd.read_csv(self.data_path, parse_dates=['date'], index_col='date')\n",
    "            \n",
    "            # æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨\n",
    "            if self.target_col not in self.df.columns:\n",
    "                raise ValueError(f\"æ•°æ®ä¸­ç¼ºå¤±ç›®æ ‡åˆ—ï¼š{self.target_col}\")\n",
    "            \n",
    "            # æ—¥å‡å€¼é‡é‡‡æ ·\n",
    "            self.df_daily_mean = self.df.resample('D').mean().dropna()\n",
    "            self.target_series = self.df_daily_mean[self.target_col].dropna()\n",
    "            \n",
    "            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§\n",
    "            if len(self.target_series) < 30:\n",
    "                raise ValueError(f\"æœ‰æ•ˆæ•°æ®ä¸è¶³30æ¡ï¼Œå½“å‰ä»…{len(self.target_series)}æ¡\")\n",
    "            \n",
    "            print(f\"æ•°æ®åŠ è½½å®Œæˆï¼šåºåˆ—é•¿åº¦={len(self.target_series)}, æ•°å€¼èŒƒå›´=[{self.target_series.min():.2f}, {self.target_series.max():.2f}] kW\")\n",
    "            return self\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"æ•°æ®åŠ è½½å¤±è´¥ï¼š{str(e)}\")\n",
    "\n",
    "    def split_train_test(self, train_ratio=0.8):\n",
    "        \"\"\"æ—¶é—´é¡ºåºåˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†\"\"\"\n",
    "        train_size = int(train_ratio * len(self.target_series))\n",
    "        self.train_series = self.target_series.iloc[:train_size]\n",
    "        self.test_series = self.target_series.iloc[train_size:]\n",
    "        print(f\"è®­ç»ƒé›†ï¼š{len(self.train_series)}æ¡æ•°æ® | æµ‹è¯•é›†ï¼š{len(self.test_series)}æ¡æ•°æ®\")\n",
    "        return self\n",
    "\n",
    "    def find_min_diff_order(self, max_diff=3):\n",
    "        \"\"\"å·®åˆ†å¹³ç¨³åŒ–ï¼Œå¯»æ‰¾æœ€å°dé˜¶\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"å·®åˆ†å¹³ç¨³åŒ–è¿‡ç¨‹\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for d in range(max_diff + 1):\n",
    "            diff_series = self.train_series.diff(d).dropna() if d > 0 else self.train_series.copy()\n",
    "            if len(diff_series) < 10:\n",
    "                continue\n",
    "            \n",
    "            adf_result = adfuller(diff_series)\n",
    "            p_value = adf_result[1]\n",
    "            print(f\"{d}é˜¶å·®åˆ†åï¼šæ•°æ®é‡={len(diff_series)} | på€¼={p_value:.4f}\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                print(f\"â†’ å¹³ç¨³ï¼ˆp<0.05ï¼‰ï¼Œæœ€å°å·®åˆ†é˜¶æ•°d={d}\")\n",
    "                return d\n",
    "        \n",
    "        print(f\"è­¦å‘Šï¼š{max_diff}é˜¶å·®åˆ†åä»éå¹³ç¨³ï¼Œä½¿ç”¨d={max_diff}\")\n",
    "        return max_diff\n",
    "\n",
    "    def auto_select_pq(self, d, max_p=3, max_q=3):\n",
    "        \"\"\"è‡ªåŠ¨é€‰æ‹©pã€qé˜¶æ•°ï¼ˆåŸºäºAICå‡†åˆ™ï¼‰\"\"\"\n",
    "        best_aic = float('inf')\n",
    "        best_pq = (0, 0)\n",
    "        \n",
    "        for p in range(max_p + 1):\n",
    "            for q in range(max_q + 1):\n",
    "                try:\n",
    "                    temp_model = ARIMA(self.train_series, order=(p, d, q))\n",
    "                    temp_result = temp_model.fit()\n",
    "                    if temp_result.aic < best_aic:\n",
    "                        best_aic = temp_result.aic\n",
    "                        best_pq = (p, q)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        p, q = best_pq\n",
    "        self.best_order = (p, d, q)\n",
    "        print(f\"\\nè‡ªåŠ¨å®šé˜¶ç»“æœï¼šp={p}, d={d}, q={q} | æœ€å°AIC={best_aic:.2f}\")\n",
    "        print(f\"æœ€ç»ˆARIMAæ¨¡å‹ï¼šARIMA{self.best_order}\")\n",
    "        return self\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"è®­ç»ƒARIMAæ¨¡å‹\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"ARIMA{self.best_order} æ¨¡å‹è®­ç»ƒ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            self.model = ARIMA(self.train_series, order=self.best_order)\n",
    "            self.model_result = self.model.fit()\n",
    "            \n",
    "            # è¾“å‡ºæ ¸å¿ƒå‚æ•°\n",
    "            print(\"\\næ¨¡å‹æ ¸å¿ƒå‚æ•°ï¼š\")\n",
    "            params_df = pd.DataFrame({\n",
    "                'å‚æ•°åç§°': self.model_result.params.index[:5],  # åªæ˜¾ç¤ºå‰5ä¸ªæ ¸å¿ƒå‚æ•°\n",
    "                'å‚æ•°å€¼': self.model_result.params.values[:5].round(4),\n",
    "                'på€¼': self.model_result.pvalues.values[:5].round(4)\n",
    "            })\n",
    "            print(params_df)\n",
    "            return self\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼š{str(e)}\")\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"è®­ç»ƒé›†æ‹Ÿåˆ+æµ‹è¯•é›†é¢„æµ‹\"\"\"\n",
    "        # è®­ç»ƒé›†å†…æ‹Ÿåˆ\n",
    "        self.in_sample_pred = self.model_result.predict(\n",
    "            start=self.train_series.index[1],\n",
    "            end=self.train_series.index[-1],\n",
    "            typ='levels'\n",
    "        )\n",
    "        self.in_sample_ci = self.model_result.get_prediction(\n",
    "            start=self.train_series.index[1],\n",
    "            end=self.train_series.index[-1]\n",
    "        ).conf_int()\n",
    "        \n",
    "        # æµ‹è¯•é›†é¢„æµ‹\n",
    "        test_forecast = self.model_result.get_forecast(steps=len(self.test_series))\n",
    "        self.test_pred = test_forecast.predicted_mean\n",
    "        self.test_ci = test_forecast.conf_int()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"è®¡ç®—è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¯¯å·®\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"è¯¯å·®è¯„ä¼°æ±‡æ€»\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # è®­ç»ƒé›†è¯„ä¼°\n",
    "        train_true = self.train_series.loc[self.in_sample_pred.index]\n",
    "        self.train_mae, self.train_rmse, self.train_mape = calculate_metrics(\n",
    "            train_true.values, self.in_sample_pred.values\n",
    "        )\n",
    "        print(f\"ã€è®­ç»ƒé›†ã€‘\")\n",
    "        print(f\"æœ‰æ•ˆæ‹Ÿåˆç‚¹æ•°ï¼š{len(self.in_sample_pred)}\")\n",
    "        print(f\"MAEï¼š{self.train_mae} kW | RMSEï¼š{self.train_rmse} kW | MAPEï¼š{self.train_mape}%\")\n",
    "        \n",
    "        # æµ‹è¯•é›†è¯„ä¼°\n",
    "        self.test_mae, self.test_rmse, self.test_mape = calculate_metrics(\n",
    "            self.test_series.values, self.test_pred.values\n",
    "        )\n",
    "        print(f\"ã€æµ‹è¯•é›†ã€‘\")\n",
    "        print(f\"é¢„æµ‹ç‚¹æ•°ï¼š{len(self.test_pred)}\")\n",
    "        print(f\"MAEï¼š{self.test_mae} kW | RMSEï¼š{self.test_rmse} kW | MAPEï¼š{self.test_mape}%\")\n",
    "        return self\n",
    "\n",
    "    def plot_result(self):\n",
    "        \"\"\"ç»˜åˆ¶è®­ç»ƒé›†æ‹Ÿåˆ+æµ‹è¯•é›†é¢„æµ‹ç»“æœ\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        \n",
    "        # ç»˜åˆ¶çœŸå®å€¼\n",
    "        ax.plot(self.target_series.index, self.target_series, color='#FF6B6B', \n",
    "                linewidth=2.5, label='çœŸå®å€¼ï¼ˆåŒºåŸŸ3ç”µåŠ›æ¶ˆè€—ï¼‰', alpha=0.8, marker='o', markersize=4)\n",
    "        \n",
    "        # ç»˜åˆ¶è®­ç»ƒé›†æ‹Ÿåˆ\n",
    "        ax.plot(self.in_sample_pred.index, self.in_sample_pred, color='#45B7D1', \n",
    "                linewidth=3, label=f'ARIMA{self.best_order} è®­ç»ƒé›†æ‹Ÿåˆ')\n",
    "        ax.fill_between(self.in_sample_ci.index, \n",
    "                        self.in_sample_ci.iloc[:, 0], self.in_sample_ci.iloc[:, 1],\n",
    "                        color='#45B7D1', alpha=0.25, label='è®­ç»ƒé›†95%ç½®ä¿¡åŒºé—´')\n",
    "        \n",
    "        # ç»˜åˆ¶æµ‹è¯•é›†é¢„æµ‹\n",
    "        ax.plot(self.test_series.index, self.test_pred, linewidth=3, color='#96CEB4',\n",
    "                label=f'ARIMA{self.best_order} æµ‹è¯•é›†é¢„æµ‹')\n",
    "        ax.fill_between(self.test_ci.index, \n",
    "                        self.test_ci.iloc[:, 0], self.test_ci.iloc[:, 1],\n",
    "                        color='#96CEB4', alpha=0.25, label='æµ‹è¯•é›†95%ç½®ä¿¡åŒºé—´')\n",
    "        \n",
    "        # åˆ†å‰²çº¿\n",
    "        ax.axvline(x=self.train_series.index[-1], color='black', linestyle=':', \n",
    "                   linewidth=2, label='è®­ç»ƒ/æµ‹è¯•é›†åˆ†å‰²çº¿')\n",
    "        \n",
    "        # å›¾è¡¨æ ·å¼\n",
    "        ax.set_title('åŒºåŸŸ3ç”µåŠ›æ¶ˆè€— - ARIMAæ¨¡å‹ è®­ç»ƒé›†æ‹Ÿåˆ+æµ‹è¯•é›†é¢„æµ‹', \n",
    "                     fontsize=18, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('æ—¥æœŸ', fontsize=14, fontweight='medium')\n",
    "        ax.set_ylabel('ç”µåŠ›æ¶ˆè€— (kW)', fontsize=14, fontweight='medium')\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=12)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # åæ ‡è½´èŒƒå›´ä¼˜åŒ–\n",
    "        y_min = self.target_series.min() * 0.95\n",
    "        y_max = self.target_series.max() * 1.05\n",
    "        x_min = self.target_series.index.min() - pd.Timedelta(days=1)\n",
    "        x_max = self.target_series.index.max() + pd.Timedelta(days=1)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        \n",
    "        ax.legend(loc='upper right', fontsize=12, framealpha=0.9)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"results/arima_train_test_fitting.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        print(\"\\nå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³ results/arima_train_test_fitting.png\")\n",
    "        return self\n",
    "\n",
    "# ä¸»å‡½æ•°\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"åŒºåŸŸ3ç”µåŠ›æ¶ˆè€—ARIMAæ—¶é—´åºåˆ—é¢„æµ‹\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # é…ç½®å‚æ•°\n",
    "    DATA_PATH = \"consumption.csv\"\n",
    "    TARGET_COL = \"Zone 3  Power Consumption\"  \n",
    "    \n",
    "    try:\n",
    "        # ä¸²è”æ•´ä¸ªæµç¨‹\n",
    "        analyzer = ARIMATimeSeriesAnalyzer(DATA_PATH, TARGET_COL)\n",
    "        analyzer.load_and_preprocess()\\\n",
    "               .split_train_test()\n",
    "               \n",
    "        d = analyzer.find_min_diff_order()\n",
    "        analyzer.auto_select_pq(d)\\\n",
    "               .train_model()\\\n",
    "               .predict()\\\n",
    "               .evaluate()\\\n",
    "               .plot_result()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"é¡¹ç›®è¿è¡Œå®Œæˆï¼\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"è¾“å‡ºæ–‡ä»¶æ¸…å•ï¼š\")\n",
    "        print(\"- é¢„æµ‹å¯è§†åŒ–å›¾ï¼šresults/arima_train_test_fitting.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nè¿è¡Œå¤±è´¥ï¼š{str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62fed0-9afe-4d1c-92e2-62ddada38375",
   "metadata": {},
   "source": [
    "## transformeræ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55b1a5e-68d7-49a8-99e7-667b23bec469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.makedirs(\"best_model\", exist_ok=True)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# è¯„ä¼°æŒ‡æ ‡å‡½æ•°\n",
    "def calculate_mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def calculate_mape(y_true, y_pred, epsilon=1e-8):\n",
    "    y_true = np.array(y_true) + epsilon\n",
    "    y_pred = np.array(y_pred)\n",
    "    return round(np.mean(np.abs((y_true - y_pred) / y_true)) * 100, 4)\n",
    "\n",
    "# æ•°æ®é›†ç±»\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len, pred_len=1):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        if len(data) < seq_len + pred_len:\n",
    "            raise ValueError(f\"æ•°æ®é•¿åº¦ä¸è¶³ï¼ˆéœ€â‰¥{seq_len+pred_len}ï¼Œå®é™…{len(data)}ï¼‰\")\n",
    "        self.length = len(data) - seq_len - pred_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.length:\n",
    "            raise IndexError(f\"ç´¢å¼•{idx}è¶…å‡ºèŒƒå›´ï¼ˆ0-{self.length-1}ï¼‰\")\n",
    "        x = self.data[idx:idx+self.seq_len]\n",
    "        y = self.data[idx+self.seq_len:idx+self.seq_len+self.pred_len]\n",
    "        return torch.tensor(x, dtype=torch.float32).unsqueeze(-1), torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "# Transformeræ¨¡å‹ç±»\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, num_heads=4, num_transformer_layers=2, pred_len=1):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        if hidden_dim % num_heads != 0:\n",
    "            raise ValueError(f\"hidden_dim({hidden_dim})å¿…é¡»èƒ½è¢«num_heads({num_heads})æ•´é™¤\")\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim * 4,\n",
    "                batch_first=True, norm_first=True, dropout=0.1\n",
    "            ),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, pred_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = self.input_proj(x)\n",
    "        x_transformer = self.transformer_encoder(x_proj)\n",
    "        out = self.fc(x_transformer[:, -1, :])\n",
    "        return out.unsqueeze(-1)\n",
    "\n",
    "# æ•°æ®åŠ è½½å‡½æ•°\n",
    "def load_data(data_path):\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_path}\")\n",
    "    \n",
    "    df = pd.read_csv(data_path, parse_dates=['DateTime'], index_col='DateTime')\n",
    "    target_col = 'Zone 3  Power Consumption'\n",
    "    \n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"æ•°æ®ç¼ºå¤±ç›®æ ‡åˆ—ï¼š{target_col}\")\n",
    "    \n",
    "    df_daily_mean = df.resample('D').mean().dropna()\n",
    "    temp_series1 = df_daily_mean[target_col].copy()\n",
    "    print(f\"æ•°æ®åŠ è½½å®Œæˆï¼šåºåˆ—é•¿åº¦={len(temp_series1)}\")\n",
    "    return temp_series1\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_data(series):\n",
    "    # ä¸€é˜¶å·®åˆ†å¹³ç¨³åŒ–\n",
    "    diff_series = series.diff(1).dropna()\n",
    "    # æ ‡å‡†åŒ–\n",
    "    scaler = StandardScaler()\n",
    "    diff_series_scaled = scaler.fit_transform(diff_series.values.reshape(-1, 1)).flatten()\n",
    "    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†\n",
    "    train_size = int(0.8 * len(diff_series_scaled))\n",
    "    train_data = diff_series_scaled[:train_size]\n",
    "    test_data = diff_series_scaled[train_size:]\n",
    "    \n",
    "    print(f\"æ•°æ®é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›†{len(train_data)}æ¡ | æµ‹è¯•é›†{len(test_data)}æ¡\")\n",
    "    return train_data, test_data, scaler, diff_series.index\n",
    "\n",
    "# é¢„æµ‹ä¸é€†å˜æ¢å‡½æ•°\n",
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            pred = model(x)\n",
    "            preds.extend(pred.cpu().numpy().flatten())\n",
    "            trues.extend(y.cpu().numpy().flatten())\n",
    "    return np.array(preds), np.array(trues)\n",
    "\n",
    "def inverse_transform(pred_scaled, scaler, series, seq_len, train_size, is_train=True):\n",
    "    pred_diff = scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "    start_idx = seq_len if is_train else train_size + seq_len\n",
    "    original_vals = series.iloc[start_idx-1 : start_idx-1 + len(pred_diff)].values\n",
    "    return original_vals + pred_diff\n",
    "\n",
    "# è®­ç»ƒè¯„ä¼°å‡½æ•°\n",
    "def train_eval(params, train_data, test_data, scaler, series, PRED_LEN, EPOCHS_BASE):\n",
    "    try:\n",
    "        train_dataset = TimeSeriesDataset(train_data, params['SEQ_LEN'], PRED_LEN)\n",
    "        test_dataset = TimeSeriesDataset(test_data, params['SEQ_LEN'], PRED_LEN)\n",
    "    except Exception as e:\n",
    "        print(f\"æ•°æ®é›†æ„å»ºå¤±è´¥ï¼š{e}\")\n",
    "        return float('inf'), float('inf'), None\n",
    "    \n",
    "    if len(train_dataset) < 10 or len(test_dataset) < 5:\n",
    "        return float('inf'), float('inf'), None\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['BATCH_SIZE'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['BATCH_SIZE'], shuffle=False)\n",
    "    \n",
    "    model = TransformerModel(\n",
    "        input_dim=1,\n",
    "        hidden_dim=params['hidden_dim'],\n",
    "        num_heads=params['num_heads'],\n",
    "        num_transformer_layers=params['num_transformer_layers'],\n",
    "        pred_len=PRED_LEN\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['LEARNING_RATE'], weight_decay=1e-5)\n",
    "    \n",
    "    # è®­ç»ƒ\n",
    "    for _ in range(EPOCHS_BASE):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "    \n",
    "    # æµ‹è¯•é›†é¢„æµ‹ä¸è¯„ä¼°\n",
    "    test_pred_scaled, test_true_scaled = predict(model, test_loader)\n",
    "    test_pred = inverse_transform(test_pred_scaled, scaler, series, params['SEQ_LEN'], len(train_data), is_train=False)\n",
    "    test_true = inverse_transform(test_true_scaled, scaler, series, params['SEQ_LEN'], len(train_data), is_train=False)\n",
    "    \n",
    "    test_mae = calculate_mae(test_true, test_pred)\n",
    "    test_mape = calculate_mape(test_true, test_pred)\n",
    "    return test_mae, test_mape, model\n",
    "\n",
    "# å¯è§†åŒ–å‡½æ•°\n",
    "def visualize_results(best_params, best_model, train_data, test_data, scaler, series, PRED_LEN):\n",
    "    # æ„å»ºæœ€ä¼˜æ¨¡å‹æ•°æ®é›†\n",
    "    best_train_dataset = TimeSeriesDataset(train_data, best_params['SEQ_LEN'], PRED_LEN)\n",
    "    best_test_dataset = TimeSeriesDataset(test_data, best_params['SEQ_LEN'], PRED_LEN)\n",
    "    best_train_loader = DataLoader(best_train_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)\n",
    "    best_test_loader = DataLoader(best_test_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)\n",
    "    \n",
    "    # é¢„æµ‹\n",
    "    train_pred_scaled, train_true_scaled = predict(best_model, best_train_loader)\n",
    "    test_pred_scaled, test_true_scaled = predict(best_model, best_test_loader)\n",
    "    \n",
    "    # é€†å˜æ¢\n",
    "    train_pred = inverse_transform(train_pred_scaled, scaler, series, best_params['SEQ_LEN'], len(train_data), is_train=True)\n",
    "    train_true = inverse_transform(train_true_scaled, scaler, series, best_params['SEQ_LEN'], len(train_data), is_train=True)\n",
    "    test_pred = inverse_transform(test_pred_scaled, scaler, series, best_params['SEQ_LEN'], len(train_data), is_train=False)\n",
    "    test_true = inverse_transform(test_true_scaled, scaler, series, best_params['SEQ_LEN'], len(train_data), is_train=False)\n",
    "    \n",
    "    # è®¡ç®—æŒ‡æ ‡\n",
    "    train_mae = calculate_mae(train_true, train_pred)\n",
    "    train_mape = calculate_mape(train_true, train_pred)\n",
    "    \n",
    "    # ç»˜åˆ¶å›¾è¡¨\n",
    "    fig, ax = plt.subplots(figsize=(18, 8))\n",
    "    train_steps = range(len(train_true))\n",
    "    test_steps = range(len(train_true), len(train_true) + len(test_true))\n",
    "    \n",
    "    ax.plot(train_steps, train_true, color='#2E86AB', linewidth=2.0, \n",
    "            label=f'è®­ç»ƒé›†çœŸå®å€¼ (MAE={train_mae:.2f} kW)', alpha=0.8)\n",
    "    ax.plot(train_steps, train_pred, color='#F2A154', linewidth=1.8, label='è®­ç»ƒé›†é¢„æµ‹å€¼', linestyle='-', alpha=0.9)\n",
    "    ax.plot(test_steps, test_true, color='#2E86AB', linewidth=2.5, \n",
    "            label=f'æµ‹è¯•é›†çœŸå®å€¼ (MAE={best_mae:.2f} kW)', alpha=0.8)\n",
    "    ax.plot(test_steps, test_pred, color='#E63946', linewidth=2.2, label='æµ‹è¯•é›†é¢„æµ‹å€¼', linestyle='--', alpha=0.9)\n",
    "    ax.axvline(x=len(train_true)-1, color='black', linestyle=':', linewidth=2.5, label='è®­ç»ƒ/æµ‹è¯•é›†åˆ†å‰²çº¿')\n",
    "    \n",
    "    ax.set_title('åŒºåŸŸ3ç”µåŠ›æ¶ˆè€— - æœ€ä¼˜çº¯Transformeræ¨¡å‹ è®­ç»ƒé›†+æµ‹è¯•é›†é¢„æµ‹ç»“æœ', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('æ—¶é—´æ­¥ï¼ˆæŒ‰æ—¥æœŸé¡ºåºï¼‰', fontsize=14), ax.set_ylabel('ç”µåŠ›æ¶ˆè€— (kW)', fontsize=14)\n",
    "    ax.tick_params(axis='x', labelsize=12), ax.tick_params(axis='y', labelsize=12)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--'), ax.legend(loc='upper right', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"best_model/transformer_train_test_prediction.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return train_mae, train_mape\n",
    "\n",
    "# ä¸»å‡½æ•°\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"åŒºåŸŸ3ç”µåŠ›æ¶ˆè€—é¢„æµ‹ï¼ˆçº¯Transformeræ¨¡å‹ï¼‰\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # é…ç½®å‚æ•°\n",
    "    DATA_PATH = \"consumption.csv\"\n",
    "    PRED_LEN = 1\n",
    "    EPOCHS_BASE = 40\n",
    "    \n",
    "    # è¶…å‚æ•°æœç´¢ç©ºé—´\n",
    "    param_grid = {\n",
    "        'SEQ_LEN': [28],\n",
    "        'hidden_dim': [32, 64, 128, 256],\n",
    "        'num_heads': [2, 4, 8, 16],\n",
    "        'num_transformer_layers': [1, 2, 3, 4],\n",
    "        'BATCH_SIZE': [8, 16, 32, 64],\n",
    "        'LEARNING_RATE': [5e-5, 1e-4, 5e-4, 1e-3]\n",
    "    }\n",
    "    \n",
    "    # ç”Ÿæˆ20ç»„æœ‰æ•ˆå‚æ•°\n",
    "    n_trials = 20\n",
    "    random_params = []\n",
    "    while len(random_params) < n_trials:\n",
    "        params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        if params['hidden_dim'] % params['num_heads'] == 0:\n",
    "            random_params.append(params)\n",
    "    \n",
    "    try:\n",
    "        # æ•°æ®åŠ è½½ä¸é¢„å¤„ç†\n",
    "        series = load_data(DATA_PATH)\n",
    "        train_data, test_data, scaler, diff_index = preprocess_data(series)\n",
    "        \n",
    "        # è¶…å‚æ•°æœç´¢\n",
    "        print(\"\\n===== å¼€å§‹20ç»„è¶…å‚æ•°éšæœºæœç´¢ =====\")\n",
    "        best_mae = float('inf')\n",
    "        best_params = None\n",
    "        best_model = None\n",
    "        \n",
    "        for i, params in enumerate(tqdm(random_params, desc=\"è°ƒå‚è¿›åº¦\")):\n",
    "            try:\n",
    "                test_mae, test_mape, model = train_eval(params, train_data, test_data, scaler, series, PRED_LEN, EPOCHS_BASE)\n",
    "                print(f\"ç¬¬{i+1}ç»„ï¼šMAE={test_mae:.6f} | MAPE={test_mape:.4f}%\")\n",
    "                \n",
    "                if test_mae < best_mae:\n",
    "                    best_mae = test_mae\n",
    "                    best_mape = test_mape\n",
    "                    best_params = params\n",
    "                    best_model = model\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"ç¬¬{i+1}ç»„å‚æ•°è®­ç»ƒå¤±è´¥ï¼š{str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # æœ€ä¼˜æ¨¡å‹ç»“æœè¾“å‡º\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"æœ€ä¼˜çº¯Transformeræ¨¡å‹ç»“æœæ±‡æ€»\")\n",
    "        print(\"=\"*100)\n",
    "        print(f\"æœ€ä¼˜å‚æ•°ç»„åˆï¼š\")\n",
    "        for k, v in best_params.items():\n",
    "            print(f\"  - {k}: {v}\")\n",
    "        print(f\"\\næµ‹è¯•é›†MAEï¼š{best_mae:.6f} kW | MAPEï¼š{best_mape:.4f}%\")\n",
    "        \n",
    "        # å¯è§†åŒ–ä¸ä¿å­˜\n",
    "        train_mae, train_mape = visualize_results(best_params, best_model, train_data, test_data, scaler, series, PRED_LEN)\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        torch.save({\n",
    "            'model_state_dict': best_model.state_dict(),\n",
    "            'params': best_params,\n",
    "            'scaler': scaler,\n",
    "            'train_mae': train_mae,\n",
    "            'train_mape': train_mape,\n",
    "            'test_mae': best_mae,\n",
    "            'test_mape': best_mape\n",
    "        }, \"best_model/best_transformer_model.pth\")\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*100)\n",
    "        print(\"å¯è§†åŒ–ä¸æ¨¡å‹ä¿å­˜å®Œæˆ\")\n",
    "        print(\"=\"*100)\n",
    "        print(f\"ğŸ“ˆ å¯è§†åŒ–å›¾å·²ä¿å­˜è‡³ï¼šbest_model/transformer_train_test_prediction.png\")\n",
    "        print(f\"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ï¼šbest_model/best_transformer_model.pth\")\n",
    "        print(f\"\\næœ€ç»ˆç»Ÿè®¡æŒ‡æ ‡ï¼š\")\n",
    "        print(f\"  - è®­ç»ƒé›† MAEï¼š{train_mae:.6f} kW | MAPEï¼š{train_mape:.4f}%\")\n",
    "        print(f\"  - æµ‹è¯•é›† MAEï¼š{best_mae:.6f} kW | MAPEï¼š{best_mape:.4f}%\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nè¿è¡Œå¤±è´¥ï¼š{str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ecbf1-4035-4da1-b598-8fc9caf7f207",
   "metadata": {},
   "source": [
    "## å•æŒ‡æ ‡Transformer+cnnæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cec934da-6669-4e2c-9395-890234f5c85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# åŸºç¡€é…ç½®\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(\"best_model\", exist_ok=True)\n",
    "os.makedirs(\"eda_plots\", exist_ok=True)\n",
    "\n",
    "# è¯„ä¼°æŒ‡æ ‡è®¡ç®—\n",
    "def calculate_mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def calculate_mape(y_true, y_pred, epsilon=1e-8):\n",
    "    y_true = np.array(y_true) + epsilon\n",
    "    y_pred = np.array(y_pred)\n",
    "    return round(np.mean(np.abs((y_true - y_pred) / y_true)) * 100, 4)\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒç±»å®šä¹‰ --------------------------\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len, pred_len=1):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        if len(data) < seq_len + pred_len:\n",
    "            raise ValueError(f\"æ•°æ®é•¿åº¦({len(data)})ä¸è¶³ï¼Œéœ€è‡³å°‘{seq_len + pred_len}ä¸ªæ ·æœ¬\")\n",
    "        self.length = len(data) - seq_len - pred_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.length:\n",
    "            raise IndexError(f\"ç´¢å¼•{idx}è¶…å‡ºèŒƒå›´ï¼ˆ0-{self.length-1}ï¼‰\")\n",
    "        x = self.data[idx:idx+self.seq_len]\n",
    "        y = self.data[idx+self.seq_len:idx+self.seq_len+self.pred_len]\n",
    "        return torch.tensor(x, dtype=torch.float32).unsqueeze(-1), torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "class CNNTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, num_heads=4, num_transformer_layers=2, kernel_size=3, pred_len=1):\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0:\n",
    "            raise ValueError(f\"hidden_dim({hidden_dim})å¿…é¡»èƒ½è¢«num_heads({num_heads})æ•´é™¤\")\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim, kernel_size, padding=kernel_size-1, dilation=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=2*(kernel_size-1), dilation=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim*4,\n",
    "                batch_first=True, norm_first=True, dropout=0.1\n",
    "            ),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, pred_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cnn = x.transpose(1, 2)\n",
    "        x_cnn = self.cnn(x_cnn).transpose(1, 2)\n",
    "        x_transformer = self.transformer_encoder(x_cnn)\n",
    "        return self.fc(x_transformer[:, -1, :]).unsqueeze(-1)\n",
    "\n",
    "# -------------------------- æ•°æ®åŠ è½½ --------------------------\n",
    "def load_public_data():\n",
    "    data_path = \"consumption.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])  \n",
    "    df.set_index(\"DateTime\", inplace=True)  \n",
    "    \n",
    "    df_daily_mean = df.resample('D').mean() \n",
    "    # æ–°å¢ï¼šæå–é¢„æµ‹ç›®æ ‡åˆ—ï¼Œä¿ç•™å•å˜é‡æ—¶é—´åºåˆ—\n",
    "    target_col = \"Zone 3  Power Consumption\"\n",
    "    df_daily_mean = df_daily_mean[[target_col]]  # åªä¿ç•™ç›®æ ‡åˆ—ï¼Œå½¢çŠ¶å˜ä¸º(364,1)\n",
    "    return df_daily_mean\n",
    "\n",
    "# -------------------------- æ¢ç´¢æ€§æ•°æ®åˆ†æ --------------------------\n",
    "def exploratory_data_analysis(df):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"æ¢ç´¢æ€§æ•°æ®åˆ†æ\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"æ•°æ®å½¢çŠ¶ï¼š{df.shape} | æ—¶é—´èŒƒå›´ï¼š{df.index.min()} è‡³ {df.index.max()}\")\n",
    "    print(\"\\næè¿°ç»Ÿè®¡ï¼š\")\n",
    "    print(df.describe().round(2))\n",
    "    \n",
    "    # æ—¶é—´åºåˆ—å›¾ï¼šä¿®å¤å¤šåˆ—æ•°æ®é—®é¢˜ï¼Œåªå–ç¬¬ä¸€åˆ—ï¼ˆç”µåŠ›æ¶ˆè€—åˆ—ï¼‰è¿›è¡Œç»˜å›¾\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    # ä¿®æ”¹ç‚¹1ï¼šä½¿ç”¨df.iloc[:, 0]è·å–å•åˆ—æ•°æ®ï¼Œé¿å…å¤šæ•°æ®é›†\n",
    "    plt.plot(df.index, df.iloc[:, 0].values, color='#2E86AB', linewidth=1.5)\n",
    "    plt.title('æ—¥å‡ç”µåŠ›æ¶ˆè€—æ—¶é—´åºåˆ—', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('æ—¥æœŸ'), plt.ylabel('ç”µåŠ›æ¶ˆè€— (kW)'), plt.grid(alpha=0.3)\n",
    "    plt.tight_layout(), plt.savefig(\"eda_plots/time_series.png\", dpi=300), plt.close()\n",
    "    \n",
    "    # åˆ†å¸ƒç›´æ–¹å›¾ï¼šä¿®å¤å¤šåˆ—æ•°æ®é—®é¢˜ï¼Œåªå–ç¬¬ä¸€åˆ—å±•å¹³ä¸ºä¸€ç»´æ•°ç»„\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # ä¿®æ”¹ç‚¹2ï¼šä½¿ç”¨df.iloc[:, 0].values.flatten()è·å–å•ç»„æ•°æ®\n",
    "    plt.hist(df.iloc[:, 0].values.flatten(), bins=30, color='#F2A154', alpha=0.7, edgecolor='black')\n",
    "    plt.title('ç”µåŠ›æ¶ˆè€—åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('ç”µåŠ›æ¶ˆè€— (kW)'), plt.ylabel('é¢‘æ•°'), plt.grid(alpha=0.3)\n",
    "    plt.tight_layout(), plt.savefig(\"eda_plots/distribution.png\", dpi=300), plt.close()\n",
    "    print(\"EDAå¯è§†åŒ–å·²ä¿å­˜è‡³ eda_plots/ ç›®å½•\")\n",
    "    return df\n",
    "\n",
    "# -------------------------- æ•°æ®é¢„å¤„ç† --------------------------\n",
    "def preprocess_data(df):\n",
    "    # ä¸€é˜¶å·®åˆ†å¹³ç¨³åŒ–ï¼ˆæ­¤æ—¶dfæ˜¯(364,1)ï¼Œå·®åˆ†åä¸º(363,1)ï¼‰\n",
    "    diff_series = df.diff(1).dropna()  # å½¢çŠ¶(363,1)\n",
    "    # æ ‡å‡†åŒ–å¤„ç†ï¼šç›´æ¥å¯¹(363,1)çš„å·®åˆ†åºåˆ—å¤„ç†ï¼Œæ— éœ€reshape(-1,1)\n",
    "    scaler = StandardScaler()\n",
    "    diff_series_scaled = scaler.fit_transform(diff_series).flatten()  # å½¢çŠ¶(363,)\n",
    "    \n",
    "    # è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ†ï¼ˆåŸºäºçœŸå®çš„363æ¡æ—¶åºæ•°æ®ï¼‰\n",
    "    train_size = int(0.8 * len(diff_series_scaled))  # 0.8Ã—363â‰ˆ290\n",
    "    train_data = diff_series_scaled[:train_size]     # çº¦290æ¡\n",
    "    test_data = diff_series_scaled[train_size:]      # çº¦73æ¡\n",
    "    \n",
    "    print(f\"æ•°æ®é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›†{len(train_data)}æ¡ | æµ‹è¯•é›†{len(test_data)}æ¡\")\n",
    "    return train_data, test_data, scaler, diff_series.index\n",
    "\n",
    "# -------------------------- è®­ç»ƒä¸è¯„ä¼° --------------------------\n",
    "def train_eval(params, train_data, test_data, scaler, df_original, pred_len, epochs):\n",
    "    try:\n",
    "        train_dataset = TimeSeriesDataset(train_data, params['SEQ_LEN'], pred_len)\n",
    "        test_dataset = TimeSeriesDataset(test_data, params['SEQ_LEN'], pred_len)\n",
    "    except Exception as e:\n",
    "        print(f\"æ•°æ®é›†æ„å»ºå¤±è´¥ï¼š{e}\")\n",
    "        return float('inf'), float('inf'), None\n",
    "    \n",
    "    if len(train_dataset) < 10 or len(test_dataset) < 5:\n",
    "        return float('inf'), float('inf'), None\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['BATCH_SIZE'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['BATCH_SIZE'], shuffle=False)\n",
    "    \n",
    "    model = CNNTransformerModel(\n",
    "        hidden_dim=params['hidden_dim'], num_heads=params['num_heads'],\n",
    "        num_transformer_layers=params['num_transformer_layers'], kernel_size=params['kernel_size'],\n",
    "        pred_len=pred_len\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['LEARNING_RATE'], weight_decay=1e-5)\n",
    "    \n",
    "    # æ¨¡å‹è®­ç»ƒ\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "    \n",
    "    # é¢„æµ‹å‡½æ•°\n",
    "    def predict(model, loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                pred = model(x)\n",
    "                preds.extend(pred.cpu().numpy().flatten())\n",
    "                trues.extend(y.cpu().numpy().flatten())\n",
    "        return np.array(preds), np.array(trues)\n",
    "    \n",
    "    # é€†å˜æ¢å‡½æ•°\n",
    "    def inverse_transform(pred_scaled, is_train=True):\n",
    "        pred_diff = scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "        start_idx = params['SEQ_LEN'] if is_train else len(train_data) + params['SEQ_LEN']\n",
    "        original_vals = df_original.iloc[start_idx-1 : start_idx-1 + len(pred_diff)].values\n",
    "        return original_vals + pred_diff\n",
    "    \n",
    "    # æµ‹è¯•é›†è¯„ä¼°\n",
    "    test_pred_scaled, test_true_scaled = predict(model, test_loader)\n",
    "    test_pred = inverse_transform(test_pred_scaled, is_train=False)\n",
    "    test_true = inverse_transform(test_true_scaled, is_train=False)\n",
    "    \n",
    "    test_mae = calculate_mae(test_true, test_pred)\n",
    "    test_mape = calculate_mape(test_true, test_pred)\n",
    "    return test_mae, test_mape, model\n",
    "\n",
    "# -------------------------- æœ€ä¼˜æ¨¡å‹å¯è§†åŒ– --------------------------\n",
    "def visualize_best_model(best_params, best_model, train_data, test_data, scaler, df_original, pred_len):\n",
    "    # æ„å»ºæ•°æ®é›†\n",
    "    best_train_dataset = TimeSeriesDataset(train_data, best_params['SEQ_LEN'], pred_len)\n",
    "    best_test_dataset = TimeSeriesDataset(test_data, best_params['SEQ_LEN'], pred_len)\n",
    "    best_train_loader = DataLoader(best_train_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)\n",
    "    best_test_loader = DataLoader(best_test_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)\n",
    "    \n",
    "    # é¢„æµ‹å‡½æ•°\n",
    "    def predict(model, loader):\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                pred = model(x)\n",
    "                preds.extend(pred.cpu().numpy().flatten())\n",
    "                trues.extend(y.cpu().numpy().flatten())\n",
    "        return np.array(preds), np.array(trues)\n",
    "    \n",
    "    # ç”Ÿæˆé¢„æµ‹ç»“æœ\n",
    "    train_pred_scaled, train_true_scaled = predict(best_model, best_train_loader)\n",
    "    test_pred_scaled, test_true_scaled = predict(best_model, best_test_loader)\n",
    "    \n",
    "    # é€†å˜æ¢\n",
    "    def inverse_transform_final(pred_scaled, is_train=True):\n",
    "        pred_diff = scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "        start_idx = best_params['SEQ_LEN'] if is_train else len(train_data) + best_params['SEQ_LEN']\n",
    "        original_vals = df_original.iloc[start_idx-1 : start_idx-1 + len(pred_diff)].values\n",
    "        return original_vals + pred_diff\n",
    "    \n",
    "    train_pred = inverse_transform_final(train_pred_scaled, is_train=True)\n",
    "    train_true = inverse_transform_final(train_true_scaled, is_train=True)\n",
    "    test_pred = inverse_transform_final(test_pred_scaled, is_train=False)\n",
    "    test_true = inverse_transform_final(test_true_scaled, is_train=False)\n",
    "    \n",
    "    # ç»˜åˆ¶å›¾è¡¨\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    train_steps = range(len(train_true))\n",
    "    test_steps = range(len(train_true), len(train_true) + len(test_true))\n",
    "    \n",
    "    ax.plot(train_steps, train_true, color='#2E86AB', linewidth=2.0, \n",
    "            label=f'è®­ç»ƒé›†çœŸå®å€¼ (MAE={calculate_mae(train_true, train_pred):.2f})', alpha=0.8)\n",
    "    ax.plot(train_steps, train_pred, color='#F2A154', linewidth=1.8, label='è®­ç»ƒé›†é¢„æµ‹å€¼', alpha=0.9)\n",
    "    ax.plot(test_steps, test_true, color='#2E86AB', linewidth=2.5, \n",
    "            label=f'æµ‹è¯•é›†çœŸå®å€¼ (MAE={calculate_mae(test_true, test_pred):.2f})', alpha=0.8)\n",
    "    ax.plot(test_steps, test_pred, color='#E63946', linewidth=2.2, label='æµ‹è¯•é›†é¢„æµ‹å€¼', linestyle='--', alpha=0.9)\n",
    "    ax.axvline(x=len(train_true)-1, color='black', linestyle=':', linewidth=2.0, label='è®­ç»ƒ/æµ‹è¯•åˆ†å‰²çº¿')\n",
    "    \n",
    "    ax.set_title('ç”µåŠ›æ¶ˆè€—é¢„æµ‹ç»“æœï¼ˆCNN-Transformerï¼‰', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('æ—¶é—´æ­¥'), ax.set_ylabel('ç”µåŠ›æ¶ˆè€— (kW)'), ax.grid(alpha=0.3), ax.legend()\n",
    "    plt.tight_layout(), plt.savefig(\"best_model/prediction_result.png\", dpi=300), plt.close()\n",
    "    print(\"é¢„æµ‹ç»“æœå¯è§†åŒ–å·²ä¿å­˜\")\n",
    "\n",
    "# -------------------------- ä¸»å‡½æ•° --------------------------\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"ç”µåŠ›æ¶ˆè€—é¢„æµ‹é¡¹ç›®\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # å…¨å±€å‚æ•°\n",
    "    PRED_LEN = 1\n",
    "    EPOCHS_BASE = 40\n",
    "    \n",
    "    # è¶…å‚æ•°æœç´¢ç©ºé—´\n",
    "    param_grid = {\n",
    "        'SEQ_LEN': [14, 28],\n",
    "        'hidden_dim': [32, 64, 128],\n",
    "        'num_heads': [2, 4, 8],\n",
    "        'num_transformer_layers': [1, 2, 3],\n",
    "        'kernel_size': [3, 5],\n",
    "        'BATCH_SIZE': [16, 32],\n",
    "        'LEARNING_RATE': [1e-4, 5e-4]\n",
    "    }\n",
    "    \n",
    "    # ç”Ÿæˆ20ç»„æœ‰æ•ˆå‚æ•°\n",
    "    n_trials = 20\n",
    "    random_params = []\n",
    "    while len(random_params) < n_trials:\n",
    "        params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        if params['hidden_dim'] % params['num_heads'] == 0:\n",
    "            random_params.append(params)\n",
    "    \n",
    "    try:\n",
    "        # æ•°æ®åŠ è½½ä¸EDA\n",
    "        df_original = load_public_data()\n",
    "        df_original = exploratory_data_analysis(df_original)\n",
    "        \n",
    "        # æ•°æ®é¢„å¤„ç†\n",
    "        train_data, test_data, scaler, diff_index = preprocess_data(df_original)\n",
    "        \n",
    "        # è¶…å‚æ•°æœç´¢\n",
    "        print(\"\\n===== å¼€å§‹20ç»„è¶…å‚æ•°æœç´¢ =====\")\n",
    "        results = []\n",
    "        for i, params in enumerate(tqdm(random_params, desc=\"è°ƒå‚è¿›åº¦\")):\n",
    "            test_mae, test_mape, model = train_eval(params, train_data, test_data, scaler, df_original, PRED_LEN, EPOCHS_BASE)\n",
    "            results.append({'params': params, 'test_mae': test_mae, 'test_mape': test_mape, 'model': model})\n",
    "            print(f\"ç¬¬{i+1}ç»„ï¼šMAE={test_mae:.6f} | MAPE={test_mape:.4f}%\")\n",
    "        \n",
    "        # ç­›é€‰æœ€ä¼˜æ¨¡å‹\n",
    "        valid_results = [r for r in results if r['test_mae'] != float('inf')]\n",
    "        if not valid_results:\n",
    "            raise ValueError(\"æ— æœ‰æ•ˆæ¨¡å‹ï¼Œè¯·æ£€æŸ¥æ•°æ®æˆ–å‚æ•°èŒƒå›´\")\n",
    "        best_result = sorted(valid_results, key=lambda x: x['test_mae'])[0]\n",
    "        best_params = best_result['params']\n",
    "        best_model = best_result['model']\n",
    "        best_mae = best_result['test_mae']\n",
    "        best_mape = best_result['test_mape']\n",
    "        \n",
    "        # æœ€ä¼˜æ¨¡å‹å¯è§†åŒ–\n",
    "        visualize_best_model(best_params, best_model, train_data, test_data, scaler, df_original, PRED_LEN)\n",
    "        \n",
    "        # ä¿å­˜æœ€ä¼˜æ¨¡å‹\n",
    "        torch.save({\n",
    "            'model_state_dict': best_model.state_dict(),\n",
    "            'params': best_params,\n",
    "            'scaler': scaler,\n",
    "            'test_mae': best_mae,\n",
    "            'test_mape': best_mape\n",
    "        }, \"best_model/best_model.pth\")\n",
    "        \n",
    "        # ç»“æœè¾“å‡º\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"æœ€ä¼˜æ¨¡å‹ç»“æœæ±‡æ€»\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"æœ€ä¼˜å‚æ•°ï¼š{best_params}\")\n",
    "        print(f\"æµ‹è¯•é›†MAEï¼š{best_mae:.6f} kW | MAPEï¼š{best_mape:.4f}%\")\n",
    "        print(f\"\\nè¾“å‡ºæ–‡ä»¶ï¼š\")\n",
    "        print(\"- EDAå¯è§†åŒ–ï¼šeda_plots/\")\n",
    "        print(\"- æœ€ä¼˜æ¨¡å‹æƒé‡ï¼šbest_model/best_model.pth\")\n",
    "        print(\"- é¢„æµ‹ç»“æœå›¾ï¼šbest_model/prediction_result.png\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nè¿è¡Œå¤±è´¥ï¼š{str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c4adb-6437-4fbe-98a8-a31a074bdb5b",
   "metadata": {},
   "source": [
    "## å¤šç‰¹å¾transformer+cnnæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5578c05-0c1d-4f76-8bbc-2c2102e7d344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "å¤šç‰¹å¾ç”µåŠ›æ¶ˆè€—é¢„æµ‹ï¼ˆZone 3ï¼‰\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "æ¢ç´¢æ€§æ•°æ®åˆ†æ\n",
      "==================================================\n",
      "æ•°æ®å½¢çŠ¶ï¼š(364, 8) | æ—¶é—´èŒƒå›´ï¼š2017-01-01 00:00:00 è‡³ 2017-12-30 00:00:00\n",
      "\n",
      "æè¿°ç»Ÿè®¡ï¼š\n",
      "       Temperature  Humidity  Wind Speed  general diffuse flows  \\\n",
      "count       364.00    364.00      364.00                 364.00   \n",
      "mean         18.81     68.26        1.96                 182.70   \n",
      "std           5.20     11.51        2.20                  86.16   \n",
      "min           8.63     29.36        0.06                  26.44   \n",
      "25%          14.20     60.03        0.08                 113.06   \n",
      "50%          19.09     70.27        0.24                 166.30   \n",
      "75%          22.87     76.94        4.91                 264.70   \n",
      "max          32.43     89.59        4.93                 335.07   \n",
      "\n",
      "       diffuse flows  Zone 1 Power Consumption  Zone 2  Power Consumption  \\\n",
      "count         364.00                    364.00                     364.00   \n",
      "mean           75.03                  32344.97                   21042.51   \n",
      "std            38.68                   2669.37                    2708.56   \n",
      "min            19.25                  26771.52                   14779.10   \n",
      "25%            44.60                  30531.19                   19046.52   \n",
      "50%            67.66                  31920.99                   20807.35   \n",
      "75%            97.55                  34100.78                   23344.34   \n",
      "max           209.13                  38733.50                   28354.54   \n",
      "\n",
      "       Zone 3  Power Consumption  \n",
      "count                     364.00  \n",
      "mean                    17835.41  \n",
      "std                      4904.52  \n",
      "min                     10530.61  \n",
      "25%                     14265.35  \n",
      "50%                     17421.57  \n",
      "75%                     18933.35  \n",
      "max                     32700.21  \n",
      "EDAå¯è§†åŒ–å·²ä¿å­˜è‡³ results/ ç›®å½•\n",
      "æ•°æ®é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›†290æ¡ | æµ‹è¯•é›†73æ¡\n",
      "\n",
      "==================================================\n",
      "CNN-Transformer è®­ç»ƒä¸è°ƒå‚\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è°ƒå‚è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æœ€ä¼˜ç»“æœï¼š\n",
      "æœ€ä¼˜å‚æ•°ï¼š{'SEQ_LEN': 14, 'hidden_dim': 64, 'num_heads': 4, 'num_transformer_layers': 2, 'kernel_size': 5, 'BATCH_SIZE': 32, 'LEARNING_RATE': 0.0005, 'epochs': 40}\n",
      "è®­ç»ƒé›†MAEï¼š333.852068 | MAPEï¼š1.7299%\n",
      "æµ‹è¯•é›†MAEï¼š481.362133 | MAPEï¼š3.7866%\n",
      "æ¨¡å‹ä¸å¯è§†åŒ–å·²ä¿å­˜è‡³ results/ ç›®å½•\n",
      "\n",
      "============================================================\n",
      "é¡¹ç›®è¿è¡Œå®Œæˆï¼\n",
      "============================================================\n",
      "è¾“å‡ºæ–‡ä»¶æ¸…å•ï¼š\n",
      "- EDAå¯è§†åŒ–ï¼šresults/ ç›®å½•ï¼ˆæ—¶é—´åºåˆ—å›¾+ç›¸å…³æ€§çƒ­åŠ›å›¾ï¼‰\n",
      "- æœ€ä¼˜æ¨¡å‹æƒé‡ï¼šresults/best_model.pth\n",
      "- é¢„æµ‹ç»“æœå›¾ï¼šresults/prediction_result.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# åŸºç¡€é…ç½®\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# è¯„ä¼°æŒ‡æ ‡è®¡ç®—\n",
    "def calculate_mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def calculate_mape(y_true, y_pred, epsilon=1e-8):\n",
    "    y_true = np.array(y_true) + epsilon\n",
    "    y_pred = np.array(y_pred)\n",
    "    return round(np.mean(np.abs((y_true - y_pred) / y_true)) * 100, 4)\n",
    "\n",
    "# æ•°æ®é›†ç±»\n",
    "class MultiFeatureTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, feat_data, target_data, seq_len, pred_len=1):\n",
    "        self.feat_data = feat_data\n",
    "        self.target_data = target_data\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        if len(feat_data) != len(target_data):\n",
    "            raise ValueError(f\"ç‰¹å¾ä¸ç›®æ ‡é•¿åº¦ä¸ä¸€è‡´ï¼ˆ{len(feat_data)} vs {len(target_data)}ï¼‰\")\n",
    "        if len(feat_data) < seq_len + pred_len:\n",
    "            raise ValueError(f\"æ•°æ®é•¿åº¦ä¸è¶³ï¼ˆéœ€â‰¥{seq_len+pred_len}ï¼Œå®é™…{len(feat_data)}ï¼‰\")\n",
    "        self.length = len(feat_data) - seq_len - pred_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.length:\n",
    "            raise IndexError(f\"ç´¢å¼•{idx}è¶…å‡ºèŒƒå›´ï¼ˆ0-{self.length-1}ï¼‰\")\n",
    "        x = self.feat_data[idx:idx+self.seq_len]\n",
    "        y = self.target_data[idx+self.seq_len:idx+self.seq_len+self.pred_len]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# æ¨¡å‹ç±»\n",
    "class CNNTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=64, num_heads=4, num_transformer_layers=2, kernel_size=3, pred_len=1):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        if hidden_dim % num_heads != 0:\n",
    "            raise ValueError(f\"hidden_dim({hidden_dim})å¿…é¡»èƒ½è¢«num_heads({num_heads})æ•´é™¤\")\n",
    "        if kernel_size % 2 == 0:\n",
    "            raise ValueError(f\"kernel_size({kernel_size})åº”ä¸ºå¥‡æ•°\")\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim, kernel_size, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim*4,\n",
    "                batch_first=True, norm_first=True, dropout=0.1\n",
    "            ),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, pred_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cnn = x.transpose(1, 2)\n",
    "        x_cnn = self.cnn(x_cnn).transpose(1, 2)\n",
    "        x_trans = self.transformer_encoder(x_cnn)\n",
    "        return self.fc(x_trans[:, -1, :]).unsqueeze(-1)\n",
    "\n",
    "# æ•°æ®åŠ è½½\n",
    "def load_public_data():\n",
    "    data_path = \"consumption.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])  \n",
    "    df.set_index(\"DateTime\", inplace=True)  \n",
    "    \n",
    "    df_daily_mean = df.resample('D').mean() \n",
    "    return df_daily_mean\n",
    "\n",
    "# æ¢ç´¢æ€§æ•°æ®åˆ†æ\n",
    "def exploratory_data_analysis(df):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"æ¢ç´¢æ€§æ•°æ®åˆ†æ\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"æ•°æ®å½¢çŠ¶ï¼š{df.shape} | æ—¶é—´èŒƒå›´ï¼š{df.index.min()} è‡³ {df.index.max()}\")\n",
    "    print(\"\\næè¿°ç»Ÿè®¡ï¼š\")\n",
    "    print(df.describe().round(2))\n",
    "    \n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print(\"\\nç¼ºå¤±å€¼å¡«å……ï¼ˆå‰å‘+åå‘ï¼‰ï¼š\")\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "        print(f\"å¡«å……åç¼ºå¤±å€¼ï¼š{df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # ç›®æ ‡å˜é‡æ—¶é—´åºåˆ—å›¾\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.plot(df.index, df['Zone 3  Power Consumption'], color='#2E86AB', linewidth=1.5)\n",
    "    plt.title('Zone 3 ç”µåŠ›æ¶ˆè€—æ—¶é—´åºåˆ—', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('æ—¥æœŸ'), plt.ylabel('ç”µåŠ›æ¶ˆè€— (kW)'), plt.grid(alpha=0.3)\n",
    "    plt.tight_layout(), plt.savefig(\"results/target_time_series.png\", dpi=300), plt.close()\n",
    "    \n",
    "    # ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    corr = df.corr()\n",
    "    plt.imshow(corr, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    plt.colorbar(label='ç›¸å…³ç³»æ•°'), plt.xticks(range(len(corr)), corr.columns, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(corr)), corr.columns), plt.title('ç‰¹å¾-ç›®æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾', fontsize=12, fontweight='bold')\n",
    "    for i in range(len(corr)):\n",
    "        for j in range(len(corr)):\n",
    "            plt.text(j, i, f'{corr.iloc[i, j]:.2f}', ha='center', va='center', fontsize=8)\n",
    "    plt.tight_layout(), plt.savefig(\"results/correlation_heatmap.png\", dpi=300), plt.close()\n",
    "    \n",
    "    print(\"EDAå¯è§†åŒ–å·²ä¿å­˜è‡³ results/ ç›®å½•\")\n",
    "    return df\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "def preprocess_data(df):\n",
    "    FEATURE_COLS = ['Temperature', 'Humidity', 'Wind Speed', 'general diffuse flows', 'diffuse flows']\n",
    "    TARGET_COL = 'Zone 3  Power Consumption'\n",
    "    \n",
    "    feat_data = df[FEATURE_COLS].values\n",
    "    target_series = df[TARGET_COL].values\n",
    "    \n",
    "    # ç›®æ ‡åºåˆ—ä¸€é˜¶å·®åˆ†å¹³ç¨³åŒ–\n",
    "    diff_target = np.diff(target_series, n=1)\n",
    "    feat_data_aligned = feat_data[1:]\n",
    "    \n",
    "    # æ ‡å‡†åŒ–å¤„ç†\n",
    "    feat_scaler = StandardScaler()\n",
    "    feat_data_scaled = feat_scaler.fit_transform(feat_data_aligned)\n",
    "    \n",
    "    target_scaler = StandardScaler()\n",
    "    diff_target_scaled = target_scaler.fit_transform(diff_target.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†\n",
    "    train_size = int(0.8 * len(feat_data_scaled))\n",
    "    train_feat, test_feat = feat_data_scaled[:train_size], feat_data_scaled[train_size:]\n",
    "    train_target, test_target = diff_target_scaled[:train_size], diff_target_scaled[train_size:]\n",
    "    \n",
    "    print(f\"æ•°æ®é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›†{len(train_feat)}æ¡ | æµ‹è¯•é›†{len(test_feat)}æ¡\")\n",
    "    return {\n",
    "        'train_feat': train_feat, 'test_feat': test_feat,\n",
    "        'train_target': train_target, 'test_target': test_target,\n",
    "        'feat_scaler': feat_scaler, 'target_scaler': target_scaler,\n",
    "        'original_target': target_series\n",
    "    }\n",
    "\n",
    "# é¢„æµ‹å‡½æ•°\n",
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            pred = model(x)\n",
    "            preds.extend(pred.cpu().numpy().flatten())\n",
    "            trues.extend(y.cpu().numpy().flatten())\n",
    "    return np.array(preds), np.array(trues)\n",
    "\n",
    "# é€†å˜æ¢å‡½æ•°\n",
    "def inverse_transform(pred_scaled, target_scaler, original_target, seq_len, train_size, is_train=True):\n",
    "    pred_diff = target_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "    start_idx = seq_len + 1\n",
    "    if not is_train:\n",
    "        start_idx += train_size\n",
    "    prev_vals = original_target[start_idx-1 : start_idx-1 + len(pred_diff)]\n",
    "    return prev_vals + pred_diff\n",
    "\n",
    "# è®­ç»ƒä¸è°ƒå‚\n",
    "def train_eval(prep_data, pred_len=1):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CNN-Transformer è®­ç»ƒä¸è°ƒå‚\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # è¶…å‚æ•°æœç´¢ç©ºé—´\n",
    "    param_grid = {\n",
    "        'SEQ_LEN': [14, 28],\n",
    "        'hidden_dim': [32, 64, 128],\n",
    "        'num_heads': [2, 4, 8],\n",
    "        'num_transformer_layers': [1, 2, 3],\n",
    "        'kernel_size': [3, 5],\n",
    "        'BATCH_SIZE': [16, 32],\n",
    "        'LEARNING_RATE': [1e-4, 5e-4],\n",
    "        'epochs': [40]\n",
    "    }\n",
    "    \n",
    "    # ç”Ÿæˆæœ‰æ•ˆå‚æ•°ç»„åˆ\n",
    "    n_trials = 20\n",
    "    valid_params = []\n",
    "    while len(valid_params) < n_trials:\n",
    "        params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        if params['hidden_dim'] % params['num_heads'] == 0:\n",
    "            valid_params.append(params)\n",
    "    \n",
    "    # æœç´¢æœ€ä¼˜å‚æ•°\n",
    "    best_mae = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    for i, params in enumerate(tqdm(valid_params, desc=\"è°ƒå‚è¿›åº¦\")):\n",
    "        try:\n",
    "            # æ„å»ºæ•°æ®é›†\n",
    "            train_dataset = MultiFeatureTimeSeriesDataset(\n",
    "                prep_data['train_feat'], prep_data['train_target'],\n",
    "                seq_len=params['SEQ_LEN'], pred_len=pred_len\n",
    "            )\n",
    "            test_dataset = MultiFeatureTimeSeriesDataset(\n",
    "                prep_data['test_feat'], prep_data['test_target'],\n",
    "                seq_len=params['SEQ_LEN'], pred_len=pred_len\n",
    "            )\n",
    "            \n",
    "            if len(train_dataset) < 10 or len(test_dataset) < 5:\n",
    "                continue\n",
    "            \n",
    "            # æ•°æ®åŠ è½½å™¨\n",
    "            train_loader = DataLoader(train_dataset, batch_size=params['BATCH_SIZE'], shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=params['BATCH_SIZE'], shuffle=False)\n",
    "            \n",
    "            # åˆå§‹åŒ–æ¨¡å‹\n",
    "            model = CNNTransformerModel(\n",
    "                input_dim=5,\n",
    "                hidden_dim=params['hidden_dim'],\n",
    "                num_heads=params['num_heads'],\n",
    "                num_transformer_layers=params['num_transformer_layers'],\n",
    "                kernel_size=params['kernel_size'],\n",
    "                pred_len=pred_len\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            # è®­ç»ƒé…ç½®\n",
    "            criterion = nn.L1Loss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['LEARNING_RATE'], weight_decay=1e-5)\n",
    "            epochs = params['epochs']\n",
    "            \n",
    "            # æ¨¡å‹è®­ç»ƒ\n",
    "            model.train()\n",
    "            for _ in range(epochs):\n",
    "                for x_batch, y_batch in train_loader:\n",
    "                    x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "                    y_pred = model(x_batch)\n",
    "                    loss = criterion(y_pred, y_batch)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # æµ‹è¯•é›†é¢„æµ‹ä¸è¯„ä¼°\n",
    "            test_pred_scaled, test_true_scaled = predict(model, test_loader)\n",
    "            test_pred = inverse_transform(\n",
    "                test_pred_scaled, prep_data['target_scaler'], prep_data['original_target'],\n",
    "                params['SEQ_LEN'], len(prep_data['train_feat']), is_train=False\n",
    "            )\n",
    "            test_true = inverse_transform(\n",
    "                test_true_scaled, prep_data['target_scaler'], prep_data['original_target'],\n",
    "                params['SEQ_LEN'], len(prep_data['train_feat']), is_train=False\n",
    "            )\n",
    "            \n",
    "            mae, mape = calculate_mae(test_true, test_pred), calculate_mape(test_true, test_pred)\n",
    "            \n",
    "            # æ›´æ–°æœ€ä¼˜æ¨¡å‹\n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                best_mape = mape\n",
    "                best_params = params\n",
    "                best_model = model\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ç¬¬{i+1}ç»„å‚æ•°è®­ç»ƒå¤±è´¥ï¼š{str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # ä¿å­˜æœ€ä¼˜æ¨¡å‹\n",
    "    torch.save({\n",
    "        'model_state_dict': best_model.state_dict(),\n",
    "        'params': best_params,\n",
    "        'feat_scaler': prep_data['feat_scaler'],\n",
    "        'target_scaler': prep_data['target_scaler'],\n",
    "        'test_mae': best_mae,\n",
    "        'test_mape': best_mape\n",
    "    }, \"results/best_model.pth\")\n",
    "    \n",
    "    # æœ€ä¼˜æ¨¡å‹å¯è§†åŒ–\n",
    "    train_dataset = MultiFeatureTimeSeriesDataset(\n",
    "        prep_data['train_feat'], prep_data['train_target'],\n",
    "        seq_len=best_params['SEQ_LEN'], pred_len=pred_len\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)\n",
    "    test_loader = DataLoader(\n",
    "        MultiFeatureTimeSeriesDataset(prep_data['test_feat'], prep_data['test_target'],\n",
    "                                     seq_len=best_params['SEQ_LEN'], pred_len=pred_len),\n",
    "        batch_size=best_params['BATCH_SIZE'], shuffle=False\n",
    "    )\n",
    "    \n",
    "    # è®­ç»ƒé›†é¢„æµ‹\n",
    "    train_pred_scaled, train_true_scaled = predict(best_model, train_loader)\n",
    "    train_pred = inverse_transform(\n",
    "        train_pred_scaled, prep_data['target_scaler'], prep_data['original_target'],\n",
    "        best_params['SEQ_LEN'], len(prep_data['train_feat']), is_train=True\n",
    "    )\n",
    "    train_true = inverse_transform(\n",
    "        train_true_scaled, prep_data['target_scaler'], prep_data['original_target'],\n",
    "        best_params['SEQ_LEN'], len(prep_data['train_feat']), is_train=True\n",
    "    )\n",
    "    \n",
    "    # è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡\n",
    "    train_mae, train_mape = calculate_mae(train_true, train_pred), calculate_mape(train_true, train_pred)\n",
    "    \n",
    "    # ç»˜åˆ¶å¯è§†åŒ–å›¾\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    train_steps = range(len(train_true))\n",
    "    test_steps = range(len(train_true), len(train_true) + len(test_true))\n",
    "    \n",
    "    ax.plot(train_steps, train_true, color='#2E86AB', linewidth=2.0, \n",
    "            label=f'è®­ç»ƒé›†çœŸå®å€¼ (MAE={train_mae:.2f} kW)', alpha=0.8)\n",
    "    ax.plot(train_steps, train_pred, color='#F2A154', linewidth=1.8, label='è®­ç»ƒé›†é¢„æµ‹å€¼', alpha=0.9)\n",
    "    ax.plot(test_steps, test_true, color='#2E86AB', linewidth=2.5, \n",
    "            label=f'æµ‹è¯•é›†çœŸå®å€¼ (MAE={best_mae:.2f} kW)', alpha=0.8)\n",
    "    ax.plot(test_steps, test_pred, color='#E63946', linewidth=2.2, label='æµ‹è¯•é›†é¢„æµ‹å€¼', linestyle='--', alpha=0.9)\n",
    "    ax.axvline(x=len(train_true)-1, color='black', linestyle=':', linewidth=2.0, label='è®­ç»ƒ/æµ‹è¯•åˆ†å‰²çº¿')\n",
    "    \n",
    "    ax.set_title('Zone 3 ç”µåŠ›æ¶ˆè€—é¢„æµ‹ç»“æœï¼ˆ5ç‰¹å¾+CNN-Transformerï¼‰', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('æ—¶é—´æ­¥'), ax.set_ylabel('ç”µåŠ›æ¶ˆè€— (kW)'), ax.grid(alpha=0.3), ax.legend()\n",
    "    plt.tight_layout(), plt.savefig(\"results/prediction_result.png\", dpi=300), plt.close()\n",
    "    \n",
    "    # è¾“å‡ºæœ€ä¼˜ç»“æœ\n",
    "    print(f\"\\næœ€ä¼˜ç»“æœï¼š\")\n",
    "    print(f\"æœ€ä¼˜å‚æ•°ï¼š{best_params}\")\n",
    "    print(f\"è®­ç»ƒé›†MAEï¼š{train_mae:.6f} | MAPEï¼š{train_mape:.4f}%\")\n",
    "    print(f\"æµ‹è¯•é›†MAEï¼š{best_mae:.6f} | MAPEï¼š{best_mape:.4f}%\")\n",
    "    print(\"æ¨¡å‹ä¸å¯è§†åŒ–å·²ä¿å­˜è‡³ results/ ç›®å½•\")\n",
    "    \n",
    "    return {\n",
    "        \"params\": best_params,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"test_mae\": best_mae,\n",
    "        \"test_mape\": best_mape\n",
    "    }\n",
    "\n",
    "# ä¸»å‡½æ•°\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"å¤šç‰¹å¾ç”µåŠ›æ¶ˆè€—é¢„æµ‹ï¼ˆZone 3ï¼‰\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # æ•°æ®åŠ è½½\n",
    "        df = load_public_data()\n",
    "        # æ¢ç´¢æ€§æ•°æ®åˆ†æ\n",
    "        df = exploratory_data_analysis(df)\n",
    "        # æ•°æ®é¢„å¤„ç†\n",
    "        prep_data = preprocess_data(df)\n",
    "        # æ¨¡å‹è®­ç»ƒä¸è°ƒå‚\n",
    "        result = train_eval(prep_data, pred_len=1)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"é¡¹ç›®è¿è¡Œå®Œæˆï¼\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"è¾“å‡ºæ–‡ä»¶æ¸…å•ï¼š\")\n",
    "        print(\"- EDAå¯è§†åŒ–ï¼šresults/ ç›®å½•ï¼ˆæ—¶é—´åºåˆ—å›¾+ç›¸å…³æ€§çƒ­åŠ›å›¾ï¼‰\")\n",
    "        print(\"- æœ€ä¼˜æ¨¡å‹æƒé‡ï¼šresults/best_model.pth\")\n",
    "        print(\"- é¢„æµ‹ç»“æœå›¾ï¼šresults/prediction_result.png\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nè¿è¡Œå¤±è´¥ï¼š{str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed4831-5bdd-486a-9e2d-4e966cb044ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88575ec9-a4b7-465b-96f2-9ab94de005ee",
   "metadata": {},
   "source": [
    "## å¤šæŒ‡æ ‡transformer+cnnè”åˆæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccfc5c53-c07d-4cfd-a48d-a28fbeabe9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.makedirs(\"multi_zone_best_model\", exist_ok=True)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# è¯„ä¼°æŒ‡æ ‡å‡½æ•°\n",
    "def calculate_mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def calculate_mape(y_true, y_pred, epsilon=1e-8):\n",
    "    y_true = np.array(y_true) + epsilon\n",
    "    y_pred = np.array(y_pred)\n",
    "    return round(np.mean(np.abs((y_true - y_pred) / y_true)) * 100, 4)\n",
    "\n",
    "def calculate_multi_zone_metrics(true_dict, pred_dict):\n",
    "    metrics = {}\n",
    "    mae_list, mape_list = [], []\n",
    "    for zone in ['z1', 'z2', 'z3']:\n",
    "        mae = calculate_mae(true_dict[zone], pred_dict[zone])\n",
    "        mape = calculate_mape(true_dict[zone], pred_dict[zone])\n",
    "        metrics[f'{zone}_mae'], metrics[f'{zone}_mape'] = mae, mape\n",
    "        mae_list.append(mae), mape_list.append(mape)\n",
    "    metrics['avg_mae'], metrics['avg_mape'] = np.mean(mae_list), np.mean(mape_list)\n",
    "    return metrics\n",
    "\n",
    "# å¤šåŒºåŸŸæ•°æ®é›†ç±»\n",
    "class MultiZoneTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, multi_zone_data, seq_len, pred_len=1):\n",
    "        self.multi_zone_data = multi_zone_data\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        if len(multi_zone_data) < seq_len + pred_len:\n",
    "            raise ValueError(f\"æ•°æ®é•¿åº¦ä¸è¶³ï¼ˆéœ€â‰¥{seq_len+pred_len}ï¼Œå®é™…{len(multi_zone_data)}ï¼‰\")\n",
    "        self.length = len(multi_zone_data) - seq_len - pred_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.length:\n",
    "            raise IndexError(f\"ç´¢å¼•{idx}è¶…å‡ºèŒƒå›´ï¼ˆ0-{self.length-1}ï¼‰\")\n",
    "        x = self.multi_zone_data[idx:idx+self.seq_len]\n",
    "        y = self.multi_zone_data[idx+self.seq_len : idx+self.seq_len+self.pred_len]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# å¤šåŒºåŸŸCNN-Transformeræ¨¡å‹ç±»\n",
    "class MultiZoneCNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=64, num_heads=4, num_transformer_layers=2, kernel_size=3, pred_len=1):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        if hidden_dim % num_heads != 0:\n",
    "            raise ValueError(f\"hidden_dim({hidden_dim})å¿…é¡»èƒ½è¢«num_heads({num_heads})æ•´é™¤\")\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim, kernel_size, padding=kernel_size-1, dilation=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=2*(kernel_size-1), dilation=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim*4,\n",
    "                batch_first=True, norm_first=True, dropout=0.1\n",
    "            ),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, pred_len * input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cnn = x.transpose(1, 2)\n",
    "        x_cnn = self.cnn(x_cnn).transpose(1, 2)\n",
    "        x_trans = self.transformer_encoder(x_cnn)\n",
    "        out = self.fc(x_trans[:, -1, :])\n",
    "        return out.reshape(-1, self.pred_len, 3)\n",
    "\n",
    "# æ•°æ®åŠ è½½å‡½æ•°ï¼ˆä¿®æ­£åï¼‰\n",
    "def load_data(data_path):\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_path}\")\n",
    "    \n",
    "    # ä¿®æ­£1ï¼šä½¿ç”¨åŸå§‹æ—¥æœŸåˆ—å\"DateTime\"ï¼Œè€Œé\"date\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    # æ‰‹åŠ¨è§£ææ—¥æœŸåˆ—å¹¶è®¾ç½®ä¸ºç´¢å¼•ï¼Œæé«˜å…¼å®¹æ€§\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"], errors='coerce')  # é”™è¯¯æ—¥æœŸè½¬ä¸ºNaT\n",
    "    df = df.dropna(subset=[\"DateTime\"])  # åˆ é™¤æ— æ•ˆæ—¥æœŸè¡Œ\n",
    "    df.set_index(\"DateTime\", inplace=True)  # è®¾ç½®ä¸ºDatetimeIndex\n",
    "    \n",
    "    # ä¿®æ­£2ï¼šç¡®ä¿ç´¢å¼•æ˜¯æ—¶é—´ç±»å‹åå†é‡é‡‡æ ·\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"ç´¢å¼•æœªæˆåŠŸè½¬ä¸ºDatetimeIndexï¼Œè¯·æ£€æŸ¥æ—¥æœŸåˆ—æ ¼å¼\")\n",
    "    \n",
    "    df_daily_mean = df.resample('D').mean().dropna()\n",
    "    \n",
    "    ZONE_COLS = {\n",
    "        'z1': 'Zone 1 Power Consumption',\n",
    "        'z2': 'Zone 2  Power Consumption',\n",
    "        'z3': 'Zone 3  Power Consumption'\n",
    "    }\n",
    "    \n",
    "    missing_cols = [col for col in ZONE_COLS.values() if col not in df_daily_mean.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"æ•°æ®ç¼ºå¤±åˆ—ï¼š{missing_cols}\")\n",
    "    \n",
    "    print(f\"æ•°æ®åŠ è½½å®Œæˆï¼šæ—¥å‡å€¼æ•°æ®å½¢çŠ¶={df_daily_mean.shape} | ç´¢å¼•ç±»å‹={type(df_daily_mean.index)}\")\n",
    "    return df_daily_mean, ZONE_COLS\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_data(df_daily_mean, ZONE_COLS):\n",
    "    # æå–åŸå§‹åºåˆ—ä¸å·®åˆ†\n",
    "    raw_zones = {}\n",
    "    diff_zones = {}\n",
    "    for zone, col in ZONE_COLS.items():\n",
    "        raw_zones[zone] = df_daily_mean[col].copy()\n",
    "        diff_zones[zone] = raw_zones[zone].diff(1).dropna()\n",
    "    \n",
    "    # å¯¹é½æ—¶é—´ç´¢å¼•\n",
    "    common_idx = diff_zones['z1'].index.intersection(diff_zones['z2'].index).intersection(diff_zones['z3'].index)\n",
    "    for zone in diff_zones.keys():\n",
    "        diff_zones[zone] = diff_zones[zone].loc[common_idx]\n",
    "    \n",
    "    # æ ‡å‡†åŒ–\n",
    "    scalers = {}\n",
    "    scaled_diff_zones = {}\n",
    "    for zone in diff_zones.keys():\n",
    "        scalers[zone] = StandardScaler()\n",
    "        scaled_diff_zones[zone] = scalers[zone].fit_transform(diff_zones[zone].values.reshape(-1,1)).flatten()\n",
    "    \n",
    "    # æ„å»ºå¤šåŒºåŸŸçŸ©é˜µ\n",
    "    multi_zone_data = np.column_stack([\n",
    "        scaled_diff_zones['z1'], scaled_diff_zones['z2'], scaled_diff_zones['z3']\n",
    "    ])\n",
    "    \n",
    "    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†\n",
    "    train_size = int(0.8 * len(multi_zone_data))\n",
    "    train_data = multi_zone_data[:train_size]\n",
    "    test_data = multi_zone_data[train_size:]\n",
    "    \n",
    "    print(f\"æ•°æ®é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›†{len(train_data)}æ¡ | æµ‹è¯•é›†{len(test_data)}æ¡\")\n",
    "    return train_data, test_data, scalers, raw_zones\n",
    "\n",
    "# é¢„æµ‹ä¸é€†å˜æ¢å‡½æ•°\n",
    "def predict_multi_zone(model, loader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            pred = model(x)\n",
    "            preds.extend(pred.cpu().numpy()), trues.extend(y.cpu().numpy())\n",
    "    return np.concatenate(preds, axis=0), np.concatenate(trues, axis=0)\n",
    "\n",
    "def inverse_transform_multi_zone(scaled_diff, zone, scalers, raw_zones, seq_len, train_size, is_train=True):\n",
    "    diff = scalers[zone].inverse_transform(scaled_diff.reshape(-1,1)).flatten()\n",
    "    start_idx = seq_len if is_train else train_size + seq_len\n",
    "    prev_raw_vals = raw_zones[zone].iloc[start_idx-1 : start_idx-1 + len(diff)].values\n",
    "    return prev_raw_vals + diff\n",
    "\n",
    "# è®­ç»ƒè¯„ä¼°å‡½æ•°\n",
    "def train_eval_multi_zone(params, train_data, test_data, PRED_LEN):\n",
    "    try:\n",
    "        train_dataset = MultiZoneTimeSeriesDataset(train_data, params['SEQ_LEN'], PRED_LEN)\n",
    "        test_dataset = MultiZoneTimeSeriesDataset(test_data, params['SEQ_LEN'], PRED_LEN)\n",
    "    except Exception as e:\n",
    "        print(f\"æ•°æ®é›†æ„å»ºå¤±è´¥ï¼š{e}\")\n",
    "        return float('inf'), {}, None\n",
    "    \n",
    "    if len(train_dataset) < 10 or len(test_dataset) < 5:\n",
    "        return float('inf'), {}, None\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['BATCH_SIZE'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['BATCH_SIZE'], shuffle=False)\n",
    "    \n",
    "    model = MultiZoneCNNTransformer(\n",
    "        input_dim=3,\n",
    "        hidden_dim=params['hidden_dim'],\n",
    "        num_heads=params['num_heads'],\n",
    "        num_transformer_layers=params['num_transformer_layers'],\n",
    "        kernel_size=params['kernel_size'],\n",
    "        pred_len=PRED_LEN\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['LEARNING_RATE'], weight_decay=1e-5)\n",
    "    EPOCHS_BASE = 40\n",
    "    \n",
    "    # è®­ç»ƒ\n",
    "    for _ in range(EPOCHS_BASE):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "    \n",
    "    # æµ‹è¯•é›†é¢„æµ‹\n",
    "    test_pred_scaled, test_true_scaled = predict_multi_zone(model, test_loader)\n",
    "    return test_pred_scaled, test_true_scaled, model\n",
    "\n",
    "# å¯è§†åŒ–å‡½æ•°\n",
    "def visualize_results(best_params, best_model, train_data, test_data, scalers, raw_zones, PRED_LEN):\n",
    "    # æ„å»ºæœ€ä¼˜æ¨¡å‹æ•°æ®é›†\n",
    "    best_train_dataset = MultiZoneTimeSeriesDataset(train_data, best_params['SEQ_LEN'], PRED_LEN)\n",
    "    best_test_dataset = MultiZoneTimeSeriesDataset(test_data, best_params['SEQ_LEN'], PRED_LEN)\n",
    "    best_train_loader = DataLoader(best_train_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)\n",
    "    best_test_loader = DataLoader(best_test_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)\n",
    "    \n",
    "    # é¢„æµ‹\n",
    "    train_pred_scaled, train_true_scaled = predict_multi_zone(best_model, best_train_loader)\n",
    "    test_pred_scaled, test_true_scaled = predict_multi_zone(best_model, best_test_loader)\n",
    "    \n",
    "    # é€†å˜æ¢\n",
    "    zone_idx = {'z1':0, 'z2':1, 'z3':2}\n",
    "    train_true, train_pred = {}, {}\n",
    "    test_true, test_pred = {}, {}\n",
    "    train_size = len(train_data)\n",
    "    \n",
    "    for zone, idx in zone_idx.items():\n",
    "        train_true[zone] = inverse_transform_multi_zone(\n",
    "            train_true_scaled[:, idx], zone, scalers, raw_zones, best_params['SEQ_LEN'], train_size, is_train=True\n",
    "        )\n",
    "        train_pred[zone] = inverse_transform_multi_zone(\n",
    "            train_pred_scaled[:, idx], zone, scalers, raw_zones, best_params['SEQ_LEN'], train_size, is_train=True\n",
    "        )\n",
    "        test_true[zone] = inverse_transform_multi_zone(\n",
    "            test_true_scaled[:, idx], zone, scalers, raw_zones, best_params['SEQ_LEN'], train_size, is_train=False\n",
    "        )\n",
    "        test_pred[zone] = inverse_transform_multi_zone(\n",
    "            test_pred_scaled[:, idx], zone, scalers, raw_zones, best_params['SEQ_LEN'], train_size, is_train=False\n",
    "        )\n",
    "    \n",
    "    # è®¡ç®—æŒ‡æ ‡\n",
    "    train_metrics = calculate_multi_zone_metrics(train_true, train_pred)\n",
    "    test_metrics = calculate_multi_zone_metrics(test_true, test_pred)\n",
    "    \n",
    "    # ç»˜åˆ¶å›¾è¡¨\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(18, 15), sharex=True)\n",
    "    zones = ['z1', 'z2', 'z3']\n",
    "    zone_names = ['Zone 1', 'Zone 2', 'Zone 3']\n",
    "    colors = ['#2E86AB', '#F2A154', '#E63946']\n",
    "    \n",
    "    for i, (zone, zone_name) in enumerate(zip(zones, zone_names)):\n",
    "        ax = axes[i]\n",
    "        train_steps = range(len(train_true[zone]))\n",
    "        test_steps = range(len(train_true[zone]), len(train_true[zone]) + len(test_true[zone]))\n",
    "        \n",
    "        ax.plot(train_steps, train_true[zone], color=colors[0], linewidth=2.0, \n",
    "                label=f'è®­ç»ƒé›†çœŸå®å€¼ (MAE={train_metrics[f\"{zone}_mae\"]:.2f} kW)', alpha=0.8)\n",
    "        ax.plot(train_steps, train_pred[zone], color=colors[1], linewidth=1.8, \n",
    "                label='è®­ç»ƒé›†é¢„æµ‹å€¼', linestyle='-', alpha=0.9)\n",
    "        ax.plot(test_steps, test_true[zone], color=colors[0], linewidth=2.5, \n",
    "                label=f'æµ‹è¯•é›†çœŸå®å€¼ (MAE={test_metrics[f\"{zone}_mae\"]:.2f} kW)', alpha=0.8)\n",
    "        ax.plot(test_steps, test_pred[zone], color=colors[2], linewidth=2.2, \n",
    "                label='æµ‹è¯•é›†é¢„æµ‹å€¼', linestyle='--', alpha=0.9)\n",
    "        ax.axvline(x=len(train_true[zone]), color='black', linestyle=':', linewidth=2.5, label='è®­ç»ƒ/æµ‹è¯•é›†åˆ†å‰²çº¿')\n",
    "        \n",
    "        ax.set_title(f'{zone_name} ç”µåŠ›æ¶ˆè€— - å¤šåŒºåŸŸè”åˆé¢„æµ‹ç»“æœ', fontsize=14, fontweight='bold', pad=10)\n",
    "        ax.set_ylabel('ç”µåŠ›æ¶ˆè€— (kW)', fontsize=12)\n",
    "        ax.tick_params(axis='x', labelsize=10), ax.tick_params(axis='y', labelsize=10)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--'), ax.legend(loc='upper right', fontsize=10)\n",
    "    \n",
    "    fig.suptitle('Zone 1/2/3 ç”µåŠ›æ¶ˆè€—å¤šåŒºåŸŸè”åˆé¢„æµ‹ç»“æœ', fontsize=18, fontweight='bold', y=0.98)\n",
    "    axes[-1].set_xlabel('æ—¶é—´æ­¥ï¼ˆæŒ‰æ—¥æœŸé¡ºåºï¼‰', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"multi_zone_best_model/multi_zone_prediction.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return train_metrics, test_metrics\n",
    "\n",
    "# ä¸»å‡½æ•°ï¼ˆä¿®æ­£åï¼‰\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"å¤šåŒºåŸŸç”µåŠ›æ¶ˆè€—è”åˆé¢„æµ‹\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # é…ç½®å‚æ•°\n",
    "    DATA_PATH = \"consumption.csv\"\n",
    "    PRED_LEN = 1\n",
    "    \n",
    "    # è¶…å‚æ•°æœç´¢ç©ºé—´ï¼ˆå…ˆå®šä¹‰ï¼Œä¸ä½¿ç”¨train_dataï¼‰\n",
    "    param_grid = {\n",
    "        'SEQ_LEN': [7, 14, 21, 28],\n",
    "        'hidden_dim': [32, 64, 128, 256],\n",
    "        'num_heads': [2, 4, 8, 16],\n",
    "        'num_transformer_layers': [1, 2, 3, 4],\n",
    "        'kernel_size': [3, 5, 7],\n",
    "        'BATCH_SIZE': [8, 16, 32, 64],\n",
    "        'LEARNING_RATE': [5e-5, 1e-4, 5e-4, 1e-3]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # ç¬¬ä¸€æ­¥ï¼šå…ˆå®Œæˆæ•°æ®åŠ è½½ä¸é¢„å¤„ç†ï¼ˆæå‰è·å–train_dataï¼‰\n",
    "        df_daily_mean, ZONE_COLS = load_data(DATA_PATH)\n",
    "        train_data, test_data, scalers, raw_zones = preprocess_data(df_daily_mean, ZONE_COLS)\n",
    "        \n",
    "        # ç¬¬äºŒæ­¥ï¼šå†ç”Ÿæˆ20ç»„æœ‰æ•ˆå‚æ•°ï¼ˆæ­¤æ—¶train_dataå·²å­˜åœ¨ï¼Œå¯æ­£å¸¸å¼•ç”¨ï¼‰\n",
    "        n_trials = 20\n",
    "        random_params = []\n",
    "        # å¢åŠ å®¹é”™ï¼šé¿å…å› train_dataè¿‡çŸ­å¯¼è‡´æ¡ä»¶æ— æ³•æ»¡è¶³\n",
    "        seq_len_upper_limit = max(1, int(len(train_data)/10))\n",
    "        print(f\"\\nç”Ÿæˆè¶…å‚æ•°æ—¶ï¼ŒSEQ_LENä¸Šé™ä¸ºï¼š{seq_len_upper_limit}ï¼ˆåŸºäºtrain_dataé•¿åº¦{len(train_data)}ï¼‰\")\n",
    "        \n",
    "        while len(random_params) < n_trials:\n",
    "            params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "            # ä¿®æ­£åˆ¤æ–­æ¡ä»¶ï¼šä½¿ç”¨å·²å­˜åœ¨çš„train_dataï¼ŒåŒæ—¶å¢åŠ seq_lenåˆæ³•æ€§åˆ¤æ–­\n",
    "            if (params['hidden_dim'] % params['num_heads'] == 0 and \n",
    "                params['SEQ_LEN'] <= seq_len_upper_limit and\n",
    "                params['SEQ_LEN'] >= 1):\n",
    "                random_params.append(params)\n",
    "        \n",
    "        # ç¬¬ä¸‰æ­¥ï¼šè¶…å‚æ•°æœç´¢ï¼ˆåç»­é€»è¾‘ä¸å˜ï¼‰\n",
    "        print(\"\\n===== å¼€å§‹20ç»„è¶…å‚æ•°éšæœºæœç´¢ =====\")\n",
    "        best_mae = float('inf')\n",
    "        best_params = None\n",
    "        best_model = None\n",
    "        \n",
    "        for i, params in enumerate(tqdm(random_params, desc=\"è°ƒå‚è¿›åº¦\")):\n",
    "            try:\n",
    "                test_pred_scaled, test_true_scaled, model = train_eval_multi_zone(params, train_data, test_data, PRED_LEN)\n",
    "                if test_pred_scaled.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                # è®¡ç®—æŒ‡æ ‡\n",
    "                test_true = {}\n",
    "                test_pred = {}\n",
    "                zone_idx = {'z1':0, 'z2':1, 'z3':2}\n",
    "                train_size = len(train_data)\n",
    "                \n",
    "                for zone, idx in zone_idx.items():\n",
    "                    test_true[zone] = inverse_transform_multi_zone(\n",
    "                        test_true_scaled[:, idx], zone, scalers, raw_zones, params['SEQ_LEN'], train_size, is_train=False\n",
    "                    )\n",
    "                    test_pred[zone] = inverse_transform_multi_zone(\n",
    "                        test_pred_scaled[:, idx], zone, scalers, raw_zones, params['SEQ_LEN'], train_size, is_train=False\n",
    "                    )\n",
    "                \n",
    "                metrics = calculate_multi_zone_metrics(test_true, test_pred)\n",
    "                print(f\"ç¬¬{i+1}ç»„ï¼šå¹³å‡MAE={metrics['avg_mae']:.6f} | å¹³å‡MAPE={metrics['avg_mape']:.4f}%\")\n",
    "                \n",
    "                if metrics['avg_mae'] < best_mae:\n",
    "                    best_mae = metrics['avg_mae']\n",
    "                    best_params = params\n",
    "                    best_model = model\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"ç¬¬{i+1}ç»„å‚æ•°è®­ç»ƒå¤±è´¥ï¼š{str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # æœ€ä¼˜æ¨¡å‹ç»“æœï¼ˆåç»­é€»è¾‘ä¸å˜ï¼‰\n",
    "        print(\"\\n\" + \"=\"*120)\n",
    "        print(\"å¤šåŒºåŸŸè”åˆé¢„æµ‹æœ€ä¼˜æ¨¡å‹ç»“æœæ±‡æ€»\")\n",
    "        print(\"=\"*120)\n",
    "        if best_params is None:\n",
    "            raise ValueError(\"æœªæ‰¾åˆ°æœ‰æ•ˆæœ€ä¼˜æ¨¡å‹ï¼Œè¯·æ£€æŸ¥å‚æ•°èŒƒå›´æˆ–æ•°æ®è´¨é‡\")\n",
    "        \n",
    "        print(f\"æœ€ä¼˜å‚æ•°ç»„åˆï¼š\")\n",
    "        for k, v in best_params.items():\n",
    "            print(f\"  - {k}: {v}\")\n",
    "        \n",
    "        # å¯è§†åŒ–ä¸ä¿å­˜\n",
    "        train_metrics, test_metrics = visualize_results(best_params, best_model, train_data, test_data, scalers, raw_zones, PRED_LEN)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š æœ€ç»ˆæŒ‡æ ‡ï¼š\")\n",
    "        print(f\"è®­ç»ƒé›† - å¹³å‡MAEï¼š{train_metrics['avg_mae']:.6f} kW | å¹³å‡MAPEï¼š{train_metrics['avg_mape']:.4f}%\")\n",
    "        print(f\"æµ‹è¯•é›† - å¹³å‡MAEï¼š{test_metrics['avg_mae']:.6f} kW | å¹³å‡MAPEï¼š{test_metrics['avg_mape']:.4f}%\")\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        torch.save({\n",
    "            'model_state_dict': best_model.state_dict(),\n",
    "            'params': best_params,\n",
    "            'scalers': scalers,\n",
    "            'raw_zones': raw_zones,\n",
    "            'train_metrics': train_metrics,\n",
    "            'test_metrics': test_metrics,\n",
    "            'zone_cols': ZONE_COLS\n",
    "        }, \"multi_zone_best_model/best_multi_zone_cnn_transformer_model.pth\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ï¼šmulti_zone_best_model/best_multi_zone_cnn_transformer_model.pth\")\n",
    "        print(f\"ğŸ“ˆ å¯è§†åŒ–å›¾å·²ä¿å­˜è‡³ï¼šmulti_zone_best_model/multi_zone_prediction.png\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nè¿è¡Œå¤±è´¥ï¼š{str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd8c2d1-86fa-4a50-a682-d90df3a44cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "å¤šåŒºåŸŸç”µåŠ›æ¶ˆè€—è”åˆé¢„æµ‹\n",
      "============================================================\n",
      "æ•°æ®åŠ è½½å®Œæˆï¼šæ—¥å‡å€¼æ•°æ®å½¢çŠ¶=(364, 8) | ç´¢å¼•ç±»å‹=<class 'pandas.core.indexes.datetimes.DatetimeIndex'>\n",
      "æ•°æ®é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›†290æ¡ | æµ‹è¯•é›†73æ¡\n",
      "\n",
      "ç”Ÿæˆè¶…å‚æ•°æ—¶ï¼ŒSEQ_LENä¸Šé™ä¸ºï¼š29ï¼ˆåŸºäºtrain_dataé•¿åº¦290ï¼‰\n",
      "\n",
      "===== å¼€å§‹20ç»„è¶…å‚æ•°éšæœºæœç´¢ =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è°ƒå‚è¿›åº¦:   0%|                                          | 0/20 [00:02<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a7a55-263e-4f55-88ab-2c07dcc316aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
