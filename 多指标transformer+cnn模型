import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
import random
from tqdm import tqdm
import os
import warnings
warnings.filterwarnings('ignore')

os.makedirs("multi_zone_best_model", exist_ok=True)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è¯„ä¼°æŒ‡æ ‡å‡½æ•°
def calculate_mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)

def calculate_mape(y_true, y_pred, epsilon=1e-8):
    y_true = np.array(y_true) + epsilon
    y_pred = np.array(y_pred)
    return round(np.mean(np.abs((y_true - y_pred) / y_true)) * 100, 4)

def calculate_multi_zone_metrics(true_dict, pred_dict):
    metrics = {}
    mae_list, mape_list = [], []
    for zone in ['z1', 'z2', 'z3']:
        mae = calculate_mae(true_dict[zone], pred_dict[zone])
        mape = calculate_mape(true_dict[zone], pred_dict[zone])
        metrics[f'{zone}_mae'], metrics[f'{zone}_mape'] = mae, mape
        mae_list.append(mae), mape_list.append(mape)
    metrics['avg_mae'], metrics['avg_mape'] = np.mean(mae_list), np.mean(mape_list)
    return metrics

# å¤šåŒºåŸŸæ•°æ®é›†ç±»
class MultiZoneTimeSeriesDataset(Dataset):
    def __init__(self, multi_zone_data, seq_len, pred_len=1):
        self.multi_zone_data = multi_zone_data
        self.seq_len = seq_len
        self.pred_len = pred_len
        
        if len(multi_zone_data) < seq_len + pred_len:
            raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼ˆéœ€â‰¥{seq_len+pred_len}ï¼Œå®é™…{len(multi_zone_data)}ï¼‰")
        self.length = len(multi_zone_data) - seq_len - pred_len + 1

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        if idx < 0 or idx >= self.length:
            raise IndexError(f"ç´¢å¼•{idx}è¶…å‡ºèŒƒå›´ï¼ˆ0-{self.length-1}ï¼‰")
        x = self.multi_zone_data[idx:idx+self.seq_len]
        y = self.multi_zone_data[idx+self.seq_len : idx+self.seq_len+self.pred_len]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

# å¤šåŒºåŸŸCNN-Transformeræ¨¡å‹ç±»
class MultiZoneCNNTransformer(nn.Module):
    def __init__(self, input_dim=3, hidden_dim=64, num_heads=4, num_transformer_layers=2, kernel_size=3, pred_len=1):
        super().__init__()
        self.pred_len = pred_len
        
        if hidden_dim % num_heads != 0:
            raise ValueError(f"hidden_dim({hidden_dim})å¿…é¡»èƒ½è¢«num_heads({num_heads})æ•´é™¤")
        
        self.cnn = nn.Sequential(
            nn.Conv1d(input_dim, hidden_dim, kernel_size, padding=kernel_size-1, dilation=1),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=2*(kernel_size-1), dilation=2),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        self.transformer_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim*4,
                batch_first=True, norm_first=True, dropout=0.1
            ),
            num_layers=num_transformer_layers
        )
        
        self.fc = nn.Linear(hidden_dim, pred_len * input_dim)

    def forward(self, x):
        x_cnn = x.transpose(1, 2)
        x_cnn = self.cnn(x_cnn).transpose(1, 2)
        x_trans = self.transformer_encoder(x_cnn)
        out = self.fc(x_trans[:, -1, :])
        return out.reshape(-1, self.pred_len, 3)

# æ•°æ®åŠ è½½å‡½æ•°ï¼ˆä¿®æ­£åï¼‰
def load_data(data_path):
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_path}")
    
    # ä¿®æ­£1ï¼šä½¿ç”¨åŸå§‹æ—¥æœŸåˆ—å"DateTime"ï¼Œè€Œé"date"
    df = pd.read_csv(data_path)
    # æ‰‹åŠ¨è§£ææ—¥æœŸåˆ—å¹¶è®¾ç½®ä¸ºç´¢å¼•ï¼Œæé«˜å…¼å®¹æ€§
    df["DateTime"] = pd.to_datetime(df["DateTime"], errors='coerce')  # é”™è¯¯æ—¥æœŸè½¬ä¸ºNaT
    df = df.dropna(subset=["DateTime"])  # åˆ é™¤æ— æ•ˆæ—¥æœŸè¡Œ
    df.set_index("DateTime", inplace=True)  # è®¾ç½®ä¸ºDatetimeIndex
    
    # ä¿®æ­£2ï¼šç¡®ä¿ç´¢å¼•æ˜¯æ—¶é—´ç±»å‹åå†é‡é‡‡æ ·
    if not isinstance(df.index, pd.DatetimeIndex):
        raise TypeError("ç´¢å¼•æœªæˆåŠŸè½¬ä¸ºDatetimeIndexï¼Œè¯·æ£€æŸ¥æ—¥æœŸåˆ—æ ¼å¼")
    
    df_daily_mean = df.resample('D').mean().dropna()
    
    ZONE_COLS = {
        'z1': 'Zone 1 Power Consumption',
        'z2': 'Zone 2  Power Consumption',
        'z3': 'Zone 3  Power Consumption'
    }
    
    missing_cols = [col for col in ZONE_COLS.values() if col not in df_daily_mean.columns]
    if missing_cols:
        raise ValueError(f"æ•°æ®ç¼ºå¤±åˆ—ï¼š{missing_cols}")
    
    print(f"æ•°æ®åŠ è½½å®Œæˆï¼šæ—¥å‡å€¼æ•°æ®å½¢çŠ¶={df_daily_mean.shape} | ç´¢å¼•ç±»å‹={type(df_daily_mean.index)}")
    return df_daily_mean, ZONE_COLS

# æ•°æ®é¢„å¤„ç†å‡½æ•°
def preprocess_data(df_daily_mean, ZONE_COLS):
    # æå–åŸå§‹åºåˆ—ä¸å·®åˆ†
    raw_zones = {}
    diff_zones = {}
    for zone, col in ZONE_COLS.items():
        raw_zones[zone] = df_daily_mean[col].copy()
        diff_zones[zone] = raw_zones[zone].diff(1).dropna()
    
    # å¯¹é½æ—¶é—´ç´¢å¼•
    common_idx = diff_zones['z1'].index.intersection(diff_zones['z2'].index).intersection(diff_zones['z3'].index)
    for zone in diff_zones.keys():
        diff_zones[zone] = diff_zones[zone].loc[common_idx]
    
    # æ ‡å‡†åŒ–
    scalers = {}
    scaled_diff_zones = {}
    for zone in diff_zones.keys():
        scalers[zone] = StandardScaler()
        scaled_diff_zones[zone] = scalers[zone].fit_transform(diff_zones[zone].values.reshape(-1,1)).flatten()
    
    # æ„å»ºå¤šåŒºåŸŸçŸ©é˜µ
    multi_zone_data = np.column_stack([
        scaled_diff_zones['z1'], scaled_diff_zones['z2'], scaled_diff_zones['z3']
    ])
    
    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    train_size = int(0.8 * len(multi_zone_data))
    train_data = multi_zone_data[:train_size]
    test_data = multi_zone_data[train_size:]
    
    print(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›†{len(train_data)}æ¡ | æµ‹è¯•é›†{len(test_data)}æ¡")
    return train_data, test_data, scalers, raw_zones

# é¢„æµ‹ä¸é€†å˜æ¢å‡½æ•°
def predict_multi_zone(model, loader):
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            pred = model(x)
            preds.extend(pred.cpu().numpy()), trues.extend(y.cpu().numpy())
    return np.concatenate(preds, axis=0), np.concatenate(trues, axis=0)

def inverse_transform_multi_zone(scaled_diff, zone, scalers, raw_zones, seq_len, train_size, is_train=True):
    diff = scalers[zone].inverse_transform(scaled_diff.reshape(-1,1)).flatten()
    start_idx = seq_len if is_train else train_size + seq_len
    prev_raw_vals = raw_zones[zone].iloc[start_idx-1 : start_idx-1 + len(diff)].values
    return prev_raw_vals + diff

# è®­ç»ƒè¯„ä¼°å‡½æ•°
def train_eval_multi_zone(params, train_data, test_data, PRED_LEN):
    try:
        train_dataset = MultiZoneTimeSeriesDataset(train_data, params['SEQ_LEN'], PRED_LEN)
        test_dataset = MultiZoneTimeSeriesDataset(test_data, params['SEQ_LEN'], PRED_LEN)
    except Exception as e:
        print(f"æ•°æ®é›†æ„å»ºå¤±è´¥ï¼š{e}")
        return float('inf'), {}, None
    
    if len(train_dataset) < 10 or len(test_dataset) < 5:
        return float('inf'), {}, None
    
    train_loader = DataLoader(train_dataset, batch_size=params['BATCH_SIZE'], shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=params['BATCH_SIZE'], shuffle=False)
    
    model = MultiZoneCNNTransformer(
        input_dim=3,
        hidden_dim=params['hidden_dim'],
        num_heads=params['num_heads'],
        num_transformer_layers=params['num_transformer_layers'],
        kernel_size=params['kernel_size'],
        pred_len=PRED_LEN
    ).to(DEVICE)
    
    criterion = nn.L1Loss()
    optimizer = optim.Adam(model.parameters(), lr=params['LEARNING_RATE'], weight_decay=1e-5)
    EPOCHS_BASE = 40
    
    # è®­ç»ƒ
    for _ in range(EPOCHS_BASE):
        model.train()
        for x_batch, y_batch in train_loader:
            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)
            y_pred = model(x_batch)
            loss = criterion(y_pred, y_batch)
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
    
    # æµ‹è¯•é›†é¢„æµ‹
    test_pred_scaled, test_true_scaled = predict_multi_zone(model, test_loader)
    return test_pred_scaled, test_true_scaled, model

# å¯è§†åŒ–å‡½æ•°
def visualize_results(best_params, best_model, train_data, test_data, scalers, raw_zones, PRED_LEN):
    # æ„å»ºæœ€ä¼˜æ¨¡å‹æ•°æ®é›†
    best_train_dataset = MultiZoneTimeSeriesDataset(train_data, best_params['SEQ_LEN'], PRED_LEN)
    best_test_dataset = MultiZoneTimeSeriesDataset(test_data, best_params['SEQ_LEN'], PRED_LEN)
    best_train_loader = DataLoader(best_train_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)
    best_test_loader = DataLoader(best_test_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)
    
    # é¢„æµ‹
    train_pred_scaled, train_true_scaled = predict_multi_zone(best_model, best_train_loader)
    test_pred_scaled, test_true_scaled = predict_multi_zone(best_model, best_test_loader)
    
    # é€†å˜æ¢
    zone_idx = {'z1':0, 'z2':1, 'z3':2}
    train_true, train_pred = {}, {}
    test_true, test_pred = {}, {}
    train_size = len(train_data)
    
    for zone, idx in zone_idx.items():
        train_true[zone] = inverse_transform_multi_zone(
            train_true_scaled[:, idx], zone, scalers, raw_zones, best_params['SEQ_LEN'], train_size, is_train=True
        )
        train_pred[zone] = inverse_transform_multi_zone(
            train_pred_scaled[:, idx], zone, scalers, raw_zones, best_params['SEQ_LEN'], train_size, is_train=True
        )
        test_true[zone] = inverse_transform_multi_zone(
            test_true_scaled[:, idx], zone, scalers, raw_zones, best_params['SEQ_LEN'], train_size, is_train=False
        )
        test_pred[zone] = inverse_transform_multi_zone(
            test_pred_scaled[:, idx], zone, scalers, raw_zones, best_params['SEQ_LEN'], train_size, is_train=False
        )
    
    # è®¡ç®—æŒ‡æ ‡
    train_metrics = calculate_multi_zone_metrics(train_true, train_pred)
    test_metrics = calculate_multi_zone_metrics(test_true, test_pred)
    
    # ç»˜åˆ¶å›¾è¡¨
    fig, axes = plt.subplots(3, 1, figsize=(18, 15), sharex=True)
    zones = ['z1', 'z2', 'z3']
    zone_names = ['Zone 1', 'Zone 2', 'Zone 3']
    colors = ['#2E86AB', '#F2A154', '#E63946']
    
    for i, (zone, zone_name) in enumerate(zip(zones, zone_names)):
        ax = axes[i]
        train_steps = range(len(train_true[zone]))
        test_steps = range(len(train_true[zone]), len(train_true[zone]) + len(test_true[zone]))
        
        ax.plot(train_steps, train_true[zone], color=colors[0], linewidth=2.0, 
                label=f'è®­ç»ƒé›†çœŸå®å€¼ (MAE={train_metrics[f"{zone}_mae"]:.2f} kW)', alpha=0.8)
        ax.plot(train_steps, train_pred[zone], color=colors[1], linewidth=1.8, 
                label='è®­ç»ƒé›†é¢„æµ‹å€¼', linestyle='-', alpha=0.9)
        ax.plot(test_steps, test_true[zone], color=colors[0], linewidth=2.5, 
                label=f'æµ‹è¯•é›†çœŸå®å€¼ (MAE={test_metrics[f"{zone}_mae"]:.2f} kW)', alpha=0.8)
        ax.plot(test_steps, test_pred[zone], color=colors[2], linewidth=2.2, 
                label='æµ‹è¯•é›†é¢„æµ‹å€¼', linestyle='--', alpha=0.9)
        ax.axvline(x=len(train_true[zone]), color='black', linestyle=':', linewidth=2.5, label='è®­ç»ƒ/æµ‹è¯•é›†åˆ†å‰²çº¿')
        
        ax.set_title(f'{zone_name} ç”µåŠ›æ¶ˆè€— - å¤šåŒºåŸŸè”åˆé¢„æµ‹ç»“æœ', fontsize=14, fontweight='bold', pad=10)
        ax.set_ylabel('ç”µåŠ›æ¶ˆè€— (kW)', fontsize=12)
        ax.tick_params(axis='x', labelsize=10), ax.tick_params(axis='y', labelsize=10)
        ax.grid(True, alpha=0.3, linestyle='--'), ax.legend(loc='upper right', fontsize=10)
    
    fig.suptitle('Zone 1/2/3 ç”µåŠ›æ¶ˆè€—å¤šåŒºåŸŸè”åˆé¢„æµ‹ç»“æœ', fontsize=18, fontweight='bold', y=0.98)
    axes[-1].set_xlabel('æ—¶é—´æ­¥ï¼ˆæŒ‰æ—¥æœŸé¡ºåºï¼‰', fontsize=14)
    plt.tight_layout()
    plt.savefig("multi_zone_best_model/multi_zone_prediction.png", bbox_inches='tight', dpi=300)
    plt.show()
    
    return train_metrics, test_metrics

# ä¸»å‡½æ•°ï¼ˆä¿®æ­£åï¼‰
def main():
    print("="*60)
    print("å¤šåŒºåŸŸç”µåŠ›æ¶ˆè€—è”åˆé¢„æµ‹")
    print("="*60)
    
    # é…ç½®å‚æ•°
    DATA_PATH = "consumption.csv"
    PRED_LEN = 1
    
    # è¶…å‚æ•°æœç´¢ç©ºé—´ï¼ˆå…ˆå®šä¹‰ï¼Œä¸ä½¿ç”¨train_dataï¼‰
    param_grid = {
        'SEQ_LEN': [7, 14, 21, 28],
        'hidden_dim': [32, 64, 128, 256],
        'num_heads': [2, 4, 8, 16],
        'num_transformer_layers': [1, 2, 3, 4],
        'kernel_size': [3, 5, 7],
        'BATCH_SIZE': [8, 16, 32, 64],
        'LEARNING_RATE': [5e-5, 1e-4, 5e-4, 1e-3]
    }
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šå…ˆå®Œæˆæ•°æ®åŠ è½½ä¸é¢„å¤„ç†ï¼ˆæå‰è·å–train_dataï¼‰
        df_daily_mean, ZONE_COLS = load_data(DATA_PATH)
        train_data, test_data, scalers, raw_zones = preprocess_data(df_daily_mean, ZONE_COLS)
        
        # ç¬¬äºŒæ­¥ï¼šå†ç”Ÿæˆ20ç»„æœ‰æ•ˆå‚æ•°ï¼ˆæ­¤æ—¶train_dataå·²å­˜åœ¨ï¼Œå¯æ­£å¸¸å¼•ç”¨ï¼‰
        n_trials = 20
        random_params = []
        # å¢åŠ å®¹é”™ï¼šé¿å…å› train_dataè¿‡çŸ­å¯¼è‡´æ¡ä»¶æ— æ³•æ»¡è¶³
        seq_len_upper_limit = max(1, int(len(train_data)/10))
        print(f"\nç”Ÿæˆè¶…å‚æ•°æ—¶ï¼ŒSEQ_LENä¸Šé™ä¸ºï¼š{seq_len_upper_limit}ï¼ˆåŸºäºtrain_dataé•¿åº¦{len(train_data)}ï¼‰")
        
        while len(random_params) < n_trials:
            params = {k: random.choice(v) for k, v in param_grid.items()}
            # ä¿®æ­£åˆ¤æ–­æ¡ä»¶ï¼šä½¿ç”¨å·²å­˜åœ¨çš„train_dataï¼ŒåŒæ—¶å¢åŠ seq_lenåˆæ³•æ€§åˆ¤æ–­
            if (params['hidden_dim'] % params['num_heads'] == 0 and 
                params['SEQ_LEN'] <= seq_len_upper_limit and
                params['SEQ_LEN'] >= 1):
                random_params.append(params)
        
        # ç¬¬ä¸‰æ­¥ï¼šè¶…å‚æ•°æœç´¢ï¼ˆåç»­é€»è¾‘ä¸å˜ï¼‰
        print("\n===== å¼€å§‹20ç»„è¶…å‚æ•°éšæœºæœç´¢ =====")
        best_mae = float('inf')
        best_params = None
        best_model = None
        
        for i, params in enumerate(tqdm(random_params, desc="è°ƒå‚è¿›åº¦")):
            try:
                test_pred_scaled, test_true_scaled, model = train_eval_multi_zone(params, train_data, test_data, PRED_LEN)
                if test_pred_scaled.size == 0:
                    continue
                
                # è®¡ç®—æŒ‡æ ‡
                test_true = {}
                test_pred = {}
                zone_idx = {'z1':0, 'z2':1, 'z3':2}
                train_size = len(train_data)
                
                for zone, idx in zone_idx.items():
                    test_true[zone] = inverse_transform_multi_zone(
                        test_true_scaled[:, idx], zone, scalers, raw_zones, params['SEQ_LEN'], train_size, is_train=False
                    )
                    test_pred[zone] = inverse_transform_multi_zone(
                        test_pred_scaled[:, idx], zone, scalers, raw_zones, params['SEQ_LEN'], train_size, is_train=False
                    )
                
                metrics = calculate_multi_zone_metrics(test_true, test_pred)
                print(f"ç¬¬{i+1}ç»„ï¼šå¹³å‡MAE={metrics['avg_mae']:.6f} | å¹³å‡MAPE={metrics['avg_mape']:.4f}%")
                
                if metrics['avg_mae'] < best_mae:
                    best_mae = metrics['avg_mae']
                    best_params = params
                    best_model = model
            
            except Exception as e:
                print(f"ç¬¬{i+1}ç»„å‚æ•°è®­ç»ƒå¤±è´¥ï¼š{str(e)}")
                continue
        
        # æœ€ä¼˜æ¨¡å‹ç»“æœï¼ˆåç»­é€»è¾‘ä¸å˜ï¼‰
        print("\n" + "="*120)
        print("å¤šåŒºåŸŸè”åˆé¢„æµ‹æœ€ä¼˜æ¨¡å‹ç»“æœæ±‡æ€»")
        print("="*120)
        if best_params is None:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆæœ€ä¼˜æ¨¡å‹ï¼Œè¯·æ£€æŸ¥å‚æ•°èŒƒå›´æˆ–æ•°æ®è´¨é‡")
        
        print(f"æœ€ä¼˜å‚æ•°ç»„åˆï¼š")
        for k, v in best_params.items():
            print(f"  - {k}: {v}")
        
        # å¯è§†åŒ–ä¸ä¿å­˜
        train_metrics, test_metrics = visualize_results(best_params, best_model, train_data, test_data, scalers, raw_zones, PRED_LEN)
        
        print(f"\nğŸ“Š æœ€ç»ˆæŒ‡æ ‡ï¼š")
        print(f"è®­ç»ƒé›† - å¹³å‡MAEï¼š{train_metrics['avg_mae']:.6f} kW | å¹³å‡MAPEï¼š{train_metrics['avg_mape']:.4f}%")
        print(f"æµ‹è¯•é›† - å¹³å‡MAEï¼š{test_metrics['avg_mae']:.6f} kW | å¹³å‡MAPEï¼š{test_metrics['avg_mape']:.4f}%")
        
        # ä¿å­˜æ¨¡å‹
        torch.save({
            'model_state_dict': best_model.state_dict(),
            'params': best_params,
            'scalers': scalers,
            'raw_zones': raw_zones,
            'train_metrics': train_metrics,
            'test_metrics': test_metrics,
            'zone_cols': ZONE_COLS
        }, "multi_zone_best_model/best_multi_zone_cnn_transformer_model.pth")
        
        print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ï¼šmulti_zone_best_model/best_multi_zone_cnn_transformer_model.pth")
        print(f"ğŸ“ˆ å¯è§†åŒ–å›¾å·²ä¿å­˜è‡³ï¼šmulti_zone_best_model/multi_zone_prediction.png")
    
    except Exception as e:
        print(f"\nè¿è¡Œå¤±è´¥ï¼š{str(e)}")
        raise
