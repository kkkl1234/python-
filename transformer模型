import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
import random
from tqdm import tqdm
import os
import warnings
warnings.filterwarnings('ignore')

os.makedirs("best_model", exist_ok=True)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è¯„ä¼°æŒ‡æ ‡å‡½æ•°
def calculate_mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)

def calculate_mape(y_true, y_pred, epsilon=1e-8):
    y_true = np.array(y_true) + epsilon
    y_pred = np.array(y_pred)
    return round(np.mean(np.abs((y_true - y_pred) / y_true)) * 100, 4)

# æ•°æ®é›†ç±»
class TimeSeriesDataset(Dataset):
    def __init__(self, data, seq_len, pred_len=1):
        self.data = data
        self.seq_len = seq_len
        self.pred_len = pred_len
        
        if len(data) < seq_len + pred_len:
            raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼ˆéœ€â‰¥{seq_len+pred_len}ï¼Œå®é™…{len(data)}ï¼‰")
        self.length = len(data) - seq_len - pred_len + 1

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        if idx < 0 or idx >= self.length:
            raise IndexError(f"ç´¢å¼•{idx}è¶…å‡ºèŒƒå›´ï¼ˆ0-{self.length-1}ï¼‰")
        x = self.data[idx:idx+self.seq_len]
        y = self.data[idx+self.seq_len:idx+self.seq_len+self.pred_len]
        return torch.tensor(x, dtype=torch.float32).unsqueeze(-1), torch.tensor(y, dtype=torch.float32).unsqueeze(-1)

# Transformeræ¨¡å‹ç±»
class TransformerModel(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64, num_heads=4, num_transformer_layers=2, pred_len=1):
        super().__init__()
        self.pred_len = pred_len
        
        if hidden_dim % num_heads != 0:
            raise ValueError(f"hidden_dim({hidden_dim})å¿…é¡»èƒ½è¢«num_heads({num_heads})æ•´é™¤")
        
        self.input_proj = nn.Linear(input_dim, hidden_dim)
        self.transformer_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim * 4,
                batch_first=True, norm_first=True, dropout=0.1
            ),
            num_layers=num_transformer_layers
        )
        self.fc = nn.Linear(hidden_dim, pred_len)

    def forward(self, x):
        x_proj = self.input_proj(x)
        x_transformer = self.transformer_encoder(x_proj)
        out = self.fc(x_transformer[:, -1, :])
        return out.unsqueeze(-1)

# æ•°æ®åŠ è½½å‡½æ•°
def load_data(data_path):
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_path}")
    
    df = pd.read_csv(data_path, parse_dates=['DateTime'], index_col='DateTime')
    target_col = 'Zone 3  Power Consumption'
    
    if target_col not in df.columns:
        raise ValueError(f"æ•°æ®ç¼ºå¤±ç›®æ ‡åˆ—ï¼š{target_col}")
    
    df_daily_mean = df.resample('D').mean().dropna()
    temp_series1 = df_daily_mean[target_col].copy()
    print(f"æ•°æ®åŠ è½½å®Œæˆï¼šåºåˆ—é•¿åº¦={len(temp_series1)}")
    return temp_series1

# æ•°æ®é¢„å¤„ç†å‡½æ•°
def preprocess_data(series):
    # ä¸€é˜¶å·®åˆ†å¹³ç¨³åŒ–
    diff_series = series.diff(1).dropna()
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    diff_series_scaled = scaler.fit_transform(diff_series.values.reshape(-1, 1)).flatten()
    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    train_size = int(0.8 * len(diff_series_scaled))
    train_data = diff_series_scaled[:train_size]
    test_data = diff_series_scaled[train_size:]
    
    print(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›†{len(train_data)}æ¡ | æµ‹è¯•é›†{len(test_data)}æ¡")
    return train_data, test_data, scaler, diff_series.index

# é¢„æµ‹ä¸é€†å˜æ¢å‡½æ•°
def predict(model, loader):
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            pred = model(x)
            preds.extend(pred.cpu().numpy().flatten())
            trues.extend(y.cpu().numpy().flatten())
    return np.array(preds), np.array(trues)

def inverse_transform(pred_scaled, scaler, series, seq_len, train_size, is_train=True):
    pred_diff = scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()
    start_idx = seq_len if is_train else train_size + seq_len
    original_vals = series.iloc[start_idx-1 : start_idx-1 + len(pred_diff)].values
    return original_vals + pred_diff

# è®­ç»ƒè¯„ä¼°å‡½æ•°
def train_eval(params, train_data, test_data, scaler, series, PRED_LEN, EPOCHS_BASE):
    try:
        train_dataset = TimeSeriesDataset(train_data, params['SEQ_LEN'], PRED_LEN)
        test_dataset = TimeSeriesDataset(test_data, params['SEQ_LEN'], PRED_LEN)
    except Exception as e:
        print(f"æ•°æ®é›†æ„å»ºå¤±è´¥ï¼š{e}")
        return float('inf'), float('inf'), None
    
    if len(train_dataset) < 10 or len(test_dataset) < 5:
        return float('inf'), float('inf'), None
    
    train_loader = DataLoader(train_dataset, batch_size=params['BATCH_SIZE'], shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=params['BATCH_SIZE'], shuffle=False)
    
    model = TransformerModel(
        input_dim=1,
        hidden_dim=params['hidden_dim'],
        num_heads=params['num_heads'],
        num_transformer_layers=params['num_transformer_layers'],
        pred_len=PRED_LEN
    ).to(DEVICE)
    
    criterion = nn.L1Loss()
    optimizer = optim.Adam(model.parameters(), lr=params['LEARNING_RATE'], weight_decay=1e-5)
    
    # è®­ç»ƒ
    for _ in range(EPOCHS_BASE):
        model.train()
        for x_batch, y_batch in train_loader:
            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)
            y_pred = model(x_batch)
            loss = criterion(y_pred, y_batch)
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
    
    # æµ‹è¯•é›†é¢„æµ‹ä¸è¯„ä¼°
    test_pred_scaled, test_true_scaled = predict(model, test_loader)
    test_pred = inverse_transform(test_pred_scaled, scaler, series, params['SEQ_LEN'], len(train_data), is_train=False)
    test_true = inverse_transform(test_true_scaled, scaler, series, params['SEQ_LEN'], len(train_data), is_train=False)
    
    test_mae = calculate_mae(test_true, test_pred)
    test_mape = calculate_mape(test_true, test_pred)
    return test_mae, test_mape, model

# å¯è§†åŒ–å‡½æ•°
def visualize_results(best_params, best_model, train_data, test_data, scaler, series, PRED_LEN):
    # æ„å»ºæœ€ä¼˜æ¨¡å‹æ•°æ®é›†
    best_train_dataset = TimeSeriesDataset(train_data, best_params['SEQ_LEN'], PRED_LEN)
    best_test_dataset = TimeSeriesDataset(test_data, best_params['SEQ_LEN'], PRED_LEN)
    best_train_loader = DataLoader(best_train_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)
    best_test_loader = DataLoader(best_test_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)
    
    # é¢„æµ‹
    train_pred_scaled, train_true_scaled = predict(best_model, best_train_loader)
    test_pred_scaled, test_true_scaled = predict(best_model, best_test_loader)
    
    # é€†å˜æ¢
    train_pred = inverse_transform(train_pred_scaled, scaler, series, best_params['SEQ_LEN'], len(train_data), is_train=True)
    train_true = inverse_transform(train_true_scaled, scaler, series, best_params['SEQ_LEN'], len(train_data), is_train=True)
    test_pred = inverse_transform(test_pred_scaled, scaler, series, best_params['SEQ_LEN'], len(train_data), is_train=False)
    test_true = inverse_transform(test_true_scaled, scaler, series, best_params['SEQ_LEN'], len(train_data), is_train=False)
    
    # è®¡ç®—æŒ‡æ ‡
    train_mae = calculate_mae(train_true, train_pred)
    train_mape = calculate_mape(train_true, train_pred)
    
    # ç»˜åˆ¶å›¾è¡¨
    fig, ax = plt.subplots(figsize=(18, 8))
    train_steps = range(len(train_true))
    test_steps = range(len(train_true), len(train_true) + len(test_true))
    
    ax.plot(train_steps, train_true, color='#2E86AB', linewidth=2.0, 
            label=f'è®­ç»ƒé›†çœŸå®å€¼ (MAE={train_mae:.2f} kW)', alpha=0.8)
    ax.plot(train_steps, train_pred, color='#F2A154', linewidth=1.8, label='è®­ç»ƒé›†é¢„æµ‹å€¼', linestyle='-', alpha=0.9)
    ax.plot(test_steps, test_true, color='#2E86AB', linewidth=2.5, 
            label=f'æµ‹è¯•é›†çœŸå®å€¼ (MAE={best_mae:.2f} kW)', alpha=0.8)
    ax.plot(test_steps, test_pred, color='#E63946', linewidth=2.2, label='æµ‹è¯•é›†é¢„æµ‹å€¼', linestyle='--', alpha=0.9)
    ax.axvline(x=len(train_true)-1, color='black', linestyle=':', linewidth=2.5, label='è®­ç»ƒ/æµ‹è¯•é›†åˆ†å‰²çº¿')
    
    ax.set_title('åŒºåŸŸ3ç”µåŠ›æ¶ˆè€— - æœ€ä¼˜çº¯Transformeræ¨¡å‹ è®­ç»ƒé›†+æµ‹è¯•é›†é¢„æµ‹ç»“æœ', fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('æ—¶é—´æ­¥ï¼ˆæŒ‰æ—¥æœŸé¡ºåºï¼‰', fontsize=14), ax.set_ylabel('ç”µåŠ›æ¶ˆè€— (kW)', fontsize=14)
    ax.tick_params(axis='x', labelsize=12), ax.tick_params(axis='y', labelsize=12)
    ax.grid(True, alpha=0.3, linestyle='--'), ax.legend(loc='upper right', fontsize=12)
    
    plt.tight_layout()
    plt.savefig("best_model/transformer_train_test_prediction.png", bbox_inches='tight', dpi=300)
    plt.show()
    
    return train_mae, train_mape

# ä¸»å‡½æ•°
def main():
    print("="*60)
    print("åŒºåŸŸ3ç”µåŠ›æ¶ˆè€—é¢„æµ‹ï¼ˆçº¯Transformeræ¨¡å‹ï¼‰")
    print("="*60)
    
    # é…ç½®å‚æ•°
    DATA_PATH = "consumption.csv"
    PRED_LEN = 1
    EPOCHS_BASE = 40
    
    # è¶…å‚æ•°æœç´¢ç©ºé—´
    param_grid = {
        'SEQ_LEN': [28],
        'hidden_dim': [32, 64, 128, 256],
        'num_heads': [2, 4, 8, 16],
        'num_transformer_layers': [1, 2, 3, 4],
        'BATCH_SIZE': [8, 16, 32, 64],
        'LEARNING_RATE': [5e-5, 1e-4, 5e-4, 1e-3]
    }
    
    # ç”Ÿæˆ20ç»„æœ‰æ•ˆå‚æ•°
    n_trials = 20
    random_params = []
    while len(random_params) < n_trials:
        params = {k: random.choice(v) for k, v in param_grid.items()}
        if params['hidden_dim'] % params['num_heads'] == 0:
            random_params.append(params)
    
    try:
        # æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
        series = load_data(DATA_PATH)
        train_data, test_data, scaler, diff_index = preprocess_data(series)
        
        # è¶…å‚æ•°æœç´¢
        print("\n===== å¼€å§‹20ç»„è¶…å‚æ•°éšæœºæœç´¢ =====")
        best_mae = float('inf')
        best_params = None
        best_model = None
        
        for i, params in enumerate(tqdm(random_params, desc="è°ƒå‚è¿›åº¦")):
            try:
                test_mae, test_mape, model = train_eval(params, train_data, test_data, scaler, series, PRED_LEN, EPOCHS_BASE)
                print(f"ç¬¬{i+1}ç»„ï¼šMAE={test_mae:.6f} | MAPE={test_mape:.4f}%")
                
                if test_mae < best_mae:
                    best_mae = test_mae
                    best_mape = test_mape
                    best_params = params
                    best_model = model
            
            except Exception as e:
                print(f"ç¬¬{i+1}ç»„å‚æ•°è®­ç»ƒå¤±è´¥ï¼š{str(e)}")
                continue
        
        # æœ€ä¼˜æ¨¡å‹ç»“æœè¾“å‡º
        print("\n" + "="*100)
        print("æœ€ä¼˜çº¯Transformeræ¨¡å‹ç»“æœæ±‡æ€»")
        print("="*100)
        print(f"æœ€ä¼˜å‚æ•°ç»„åˆï¼š")
        for k, v in best_params.items():
            print(f"  - {k}: {v}")
        print(f"\næµ‹è¯•é›†MAEï¼š{best_mae:.6f} kW | MAPEï¼š{best_mape:.4f}%")
        
        # å¯è§†åŒ–ä¸ä¿å­˜
        train_mae, train_mape = visualize_results(best_params, best_model, train_data, test_data, scaler, series, PRED_LEN)
        
        # ä¿å­˜æ¨¡å‹
        torch.save({
            'model_state_dict': best_model.state_dict(),
            'params': best_params,
            'scaler': scaler,
            'train_mae': train_mae,
            'train_mape': train_mape,
            'test_mae': best_mae,
            'test_mape': best_mape
        }, "best_model/best_transformer_model.pth")
        
        print(f"\n" + "="*100)
        print("å¯è§†åŒ–ä¸æ¨¡å‹ä¿å­˜å®Œæˆ")
        print("="*100)
        print(f"ğŸ“ˆ å¯è§†åŒ–å›¾å·²ä¿å­˜è‡³ï¼šbest_model/transformer_train_test_prediction.png")
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ï¼šbest_model/best_transformer_model.pth")
        print(f"\næœ€ç»ˆç»Ÿè®¡æŒ‡æ ‡ï¼š")
        print(f"  - è®­ç»ƒé›† MAEï¼š{train_mae:.6f} kW | MAPEï¼š{train_mape:.4f}%")
        print(f"  - æµ‹è¯•é›† MAEï¼š{best_mae:.6f} kW | MAPEï¼š{best_mape:.4f}%")
    
    except Exception as e:
        print(f"\nè¿è¡Œå¤±è´¥ï¼š{str(e)}")
        raise

if __name__ == "__main__":
    main()
